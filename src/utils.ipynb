{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f204ad",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "This notebook contains shared utility functions for data preparation. It allows me to access and reuse data-related functionality across different notebooks without duplicating code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef9d96",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd737aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8b9de",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "To streamline training and validation, I implemented a custom PyTorch `Dataset` class for convenient data access.\n",
    "\n",
    "**Data Augmentation**\n",
    "\n",
    "To improve generalization, I applied the following augmentation techniques:\n",
    "\n",
    "- Random horizontal flip\n",
    "- Random rotation\n",
    "- Random height and width shift\n",
    "\n",
    "I intentionally excluded vertical flipping, as it would alter the semantics of some classes (e.g., upside-down baskets would lose their recognizable structure).\n",
    "\n",
    "Given the low resolution and grayscale nature of the images, I chose not to add noise or apply color-based transformations.\n",
    "\n",
    "**Dataset Overview**\n",
    "\n",
    "- **Classes:** 5 (baskets, eyes, binoculars, rabbits, hands)\n",
    "- **Training set:** 10,000 images per class (50,000 total)\n",
    "- **Test set:** 5,000 images per class (25,000 total)\n",
    "- **Image dimensions:** 28×28 pixels (784 features per image)\n",
    "\n",
    "To improve training stability and convergence speed, I also computed the **mean and standard deviation** of the training set. These statistics were then used to normalize the input images. This ensures that the pixel values are centered around zero with unit variance, which is especially beneficial for models using gradient-based optimization methods like SGD or Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13dee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for QuickDraw\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, class_label=None):\n",
    "        self.img_labels = pl.read_csv(annotations_file)\n",
    "        if class_label is not None:\n",
    "            self.img_labels = self.img_labels.filter(pl.col(\"class_label\") == class_label)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.img_labels.row(idx)[1:]\n",
    "        img_path = os.path.join(self.img_dir, img_path)\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66670acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.1982])\n",
      "Std: tensor([0.3426])\n"
     ]
    }
   ],
   "source": [
    "# Calculate  mean and std for normalization\n",
    "norm_dataset = QuickDrawDataset('../dataset/train.csv', '../dataset/images', transform=v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]))\n",
    "loader = DataLoader(norm_dataset, batch_size=64, shuffle=False, num_workers=6)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0\n",
    "\n",
    "for data, _ in loader:\n",
    "    batch_samples = data.size(0)  # batch size (64 here)\n",
    "    data = data.view(batch_samples, data.size(1), -1)  # flatten H and W\n",
    "    mean += data.mean(2).sum(0)  # mean per channel summed over batch\n",
    "    std += data.std(2).sum(0)    # std per channel summed over batch\n",
    "    nb_samples += batch_samples\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "\n",
    "print('Mean:', mean)\n",
    "print('Std:', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6c10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation transforms\n",
    "train_transforms = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.RandomHorizontalFlip(p = 0.5),\n",
    "    v2.RandomRotation(degrees = 10),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Width and height shift\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "test_transforms = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Classification targets & subfolder names\n",
    "classes = {\n",
    "    0: 'basket',\n",
    "    1: 'eye',\n",
    "    2: 'binoculars',\n",
    "    3: 'rabbit',\n",
    "    4: 'hand',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5391ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train and Test Datasets\n",
    "train_data = QuickDrawDataset('../dataset/train.csv', '../dataset/images', train_transforms)\n",
    "test_data = QuickDrawDataset('../dataset/test.csv', '../dataset/images', test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed6fa6",
   "metadata": {},
   "source": [
    "## Base Module\n",
    "\n",
    "For the classifier model, I wanted to experiment with different architectural variations and evaluate their performance. To facilitate this, I created a `BaseModule` class that provides a common structure. This base can be extended with an arbitrary number of layers, making it easy to compare different configurations systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7170b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d88840",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "To speed up training, I initialized a common device that automatically uses CUDA or MPS acceleration if available. This ensures the models run on GPU when possible, falling back to CPU only if no hardware acceleration is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1447f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef67b3",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "To visually inspect the classification model’s performance, I implemented a sampling function that randomly selects *n* images per class. Each image is returned as a tuple containing its relative file path and the corresponding class label. This allows for quick, class-balanced sampling from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25810c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample a number of images of each class from the test data\n",
    "def sample_image_from_each_class(n=1):\n",
    "    df = pl.read_csv(\"../dataset/test.csv\")\n",
    "    sampled_images = df.group_by(\"class_label\").map_groups(lambda group: group.sample(n))\n",
    "    return list(zip(sampled_images[\"class_label\"].to_list(), sampled_images[\"relative_path\"].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433e354-8e33-47a5-9cf4-851f55a76948",
   "metadata": {},
   "source": [
    "## Classwise Dataset\n",
    "\n",
    "Since training a conditional generative model did not yield the expected results, I decided to use a classwise dataset instead. To achieve this, I extended the original dataset class I created earlier by adding functionality to filter and load only images belonging to a specific class. This allows me to train separate generative models for each class individually and potentially achieve better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd473db0-bd9c-48f9-8dc5-a05bd00a6dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set should contain 5000 images of class 4 (hand)\n",
    "test = QuickDrawDataset('../dataset/test.csv', '../dataset/images', class_label=4)\n",
    "len(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
