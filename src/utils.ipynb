{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f204ad",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "This notebook contains shared functions for data preparation. This way i can reference the data without rewriting or copy pasting the functions twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd737aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8b9de",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First of all we need to properly prepare our data. Therefore I implemented a PyTorch Dataset for the data to be conveniently accessed during training and validation.\n",
    "\n",
    "Moreover I performed some data augmentation, namely i applied:\n",
    "\n",
    "    - random horizontal flip\n",
    "    - random rotation\n",
    "    - random height and width shift\n",
    "\n",
    "I choose to not use a vertical flip, because for example in the case of baskets this would destroy the meaning of the image.\n",
    "Since the images are very low resolution gray scale images I decided to not apply any additional noise and color transforms.\n",
    "\n",
    "Stats of the data:\n",
    "\n",
    "    - 5 classes\n",
    "    - 10.000 images in training set per class (50k total)\n",
    "    - 5.000 images per class in test set (25k total)\n",
    "    - 28 x 28 images -> 784 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13dee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for QuickDraw\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, class_label=None):\n",
    "        self.img_labels = pl.read_csv(annotations_file)\n",
    "        if class_label is not None:\n",
    "            self.img_labels = self.img_labels.filter(pl.col(\"class_label\") == class_label)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.img_labels.row(idx)[1:]\n",
    "        img_path = os.path.join(self.img_dir, img_path)\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66670acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.1982])\n",
      "Std: tensor([0.3426])\n"
     ]
    }
   ],
   "source": [
    "# Calculate  mean and std for normalization\n",
    "norm_dataset = QuickDrawDataset('../dataset/train.csv', '../dataset/images', transform=v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]))\n",
    "loader = DataLoader(norm_dataset, batch_size=64, shuffle=False, num_workers=6)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0\n",
    "\n",
    "for data, _ in loader:\n",
    "    batch_samples = data.size(0)  # batch size (64 here)\n",
    "    data = data.view(batch_samples, data.size(1), -1)  # flatten H and W\n",
    "    mean += data.mean(2).sum(0)  # mean per channel summed over batch\n",
    "    std += data.std(2).sum(0)    # std per channel summed over batch\n",
    "    nb_samples += batch_samples\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "\n",
    "print('Mean:', mean)\n",
    "print('Std:', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6c10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation transforms\n",
    "train_transforms = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.RandomHorizontalFlip(p = 0.5),\n",
    "    v2.RandomRotation(degrees = 10),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Width and height shift\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "test_transforms = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Classification targets & subfolder names\n",
    "classes = {\n",
    "    0: 'basket',\n",
    "    1: 'eye',\n",
    "    2: 'binoculars',\n",
    "    3: 'rabbit',\n",
    "    4: 'hand',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5391ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train and Test Datasets\n",
    "train_data = QuickDrawDataset('../dataset/train.csv', '../dataset/images', train_transforms)\n",
    "test_data = QuickDrawDataset('../dataset/test.csv', '../dataset/images', test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed6fa6",
   "metadata": {},
   "source": [
    "## Base Module\n",
    "\n",
    "The least for the classifier model I want to construct different model architectures and evaluate them against each other. The later code serves a common base to add an arbitrary amount of layers to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7170b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d88840",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "Initialize a common device to speed up  training if mpu or cuda acceleration is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1447f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef67b3",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "In order to visually test the classification model I created a function that provides me a n images of each class as a tuple of their relative path and their class label at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25810c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample a number of images of each class from the test data\n",
    "def sample_image_from_each_class(n=1):\n",
    "    df = pl.read_csv(\"../dataset/test.csv\")\n",
    "    sampled_images = df.group_by(\"class_label\").map_groups(lambda group: group.sample(n))\n",
    "    return list(zip(sampled_images[\"class_label\"].to_list(), sampled_images[\"relative_path\"].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433e354-8e33-47a5-9cf4-851f55a76948",
   "metadata": {},
   "source": [
    "## Classwise Dataset\n",
    "\n",
    "Since training conditional generative model did provide the output I expect, I will use this classwise dataset that only loads the image of a particular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd473db0-bd9c-48f9-8dc5-a05bd00a6dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = QuickDrawDataset('../dataset/test.csv', '../dataset/images', class_label=4)\n",
    "len(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
