{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f204ad",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "This notebook contains shared functions for data preparation. This way i can reference the data without rewriting or copy pasting the functions twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd737aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8b9de",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First of all we need to properly prepare our data. Therefore I implemented a PyTorch Dataset for the data to be conveniently accessed during training and validation.\n",
    "\n",
    "Moreover I performed some data augmentation, namely i applied:\n",
    "\n",
    "    - random horizontal flip\n",
    "    - random rotation\n",
    "    - random height and width shift\n",
    "\n",
    "I choose to not use a vertical flip, because for example in the case of baskets this would destroy the meaning of the image.\n",
    "Since the images are very low resolution gray scale images I decided to not apply any additional noise and color transforms.\n",
    "\n",
    "Stats of the data:\n",
    "\n",
    "    - 5 classes\n",
    "    - 10.000 images in training set per class (50k total)\n",
    "    - 5.000 images per class in test set (25k total)\n",
    "    - 28 x 28 images -> 784 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation transforms\n",
    "train_transforms = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.RandomHorizontalFlip(p = 0.5),\n",
    "    v2.RandomRotation(degrees = 10),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Width and height shift\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "test_transforms = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "# Classification targets & subfolder names\n",
    "classes = {\n",
    "    0: 'basket',\n",
    "    1: 'eye',\n",
    "    2: 'binoculars',\n",
    "    3: 'rabbit',\n",
    "    4: 'hand',\n",
    "}\n",
    "\n",
    "# Dataset class for QuickDraw\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = pl.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.img_labels.row(idx)[1:]\n",
    "        img_path = os.path.join(self.img_dir, img_path)\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Create Train and Test Datasets\n",
    "train_data = QuickDrawDataset('../dataset/train.csv', '../dataset/images', train_transforms)\n",
    "test_data = QuickDrawDataset('../dataset/test.csv', '../dataset/images', test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed6fa6",
   "metadata": {},
   "source": [
    "## Base Module\n",
    "\n",
    "The least for the classifier model I want to construct different model architectures and evaluate them against each other. The later code serves a common base to add an arbitrary amount of layers to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7170b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d88840",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "Initialize a common device to speed up  training if mpu or cuda acceleration is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1447f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
