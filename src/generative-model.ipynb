{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929afcb6",
   "metadata": {},
   "source": [
    "# Generative Model\n",
    "\n",
    "This notebook deals with finding a good generative model.\n",
    "\n",
    "# TODO: VAE, Diffusion Model & (Sharpening U-Net GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import import_ipynb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils import DEVICE, classes, train_data, test_data, BaseModule, train_transforms, QuickDrawDataset # type: ignore\n",
    "import os\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "354591b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 40\n",
    "PATIENCE = 20\n",
    "\n",
    "# Data loaders\n",
    "TRAIN_LOADER = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=6)\n",
    "TEST_LOADER = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=6)\n",
    "\n",
    "# LR scheduler lambda function\n",
    "def lr_lambda(epoch):\n",
    "    return 0.95 ** (epoch - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a13d3e",
   "metadata": {},
   "source": [
    "## Conditional Variatonal Autoencoder\n",
    "\n",
    "First of all I want to start with a CNN-based variatonal Autoencoder. This way I can apply the knowledge that I gained while building the Classifier Model earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce2e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE loss function - combines reconstruction loss and KL divergence\n",
    "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
    "    BCE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE, KLD, (BCE + beta * KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a45322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conditional Variational Autoencoder (CVAE) model\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=50, num_classes=len(classes)):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # label embedding to inject the information into the encoder\n",
    "        self.label_embed_conv = nn.Linear(num_classes, 28 * 28)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(2, 24, kernel_size=3, padding=1),  # 1 channel image + 1 channel label map\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.Conv2d(24, 24, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(24, 48, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.Conv2d(48, 48, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "\n",
    "        # Latent mappings\n",
    "        self.fc_input_dim = 48 * 7 * 7\n",
    "        self.fc_mu = nn.Linear(self.fc_input_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.fc_input_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim + num_classes, 48 * 7 * 7)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48, 48, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(48),\n",
    "\n",
    "            nn.ConvTranspose2d(48, 24, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(24),\n",
    "\n",
    "            nn.Conv2d(24, 1, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    # Reparameterization trick to sample from the latent space\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # One-hot label to image-size label map\n",
    "        y_onehot = F.one_hot(y, self.num_classes).float().to(x.device)\n",
    "        y_map = self.label_embed_conv(y_onehot).view(batch_size, 1, 28, 28)\n",
    "\n",
    "        # Concatenate image and label map\n",
    "        x_cat = torch.cat([x, y_map], dim=1)\n",
    "\n",
    "        x_encoded = self.encoder(x_cat)\n",
    "        x_flat = x_encoded.view(batch_size, -1)\n",
    "\n",
    "        mu = self.fc_mu(x_flat)\n",
    "        logvar = self.fc_logvar(x_flat)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode\n",
    "        z_cat = torch.cat([z, y_onehot], dim=1)\n",
    "        x_decoded = self.fc_decode(z_cat)\n",
    "        x_decoded = x_decoded.view(batch_size, 48, 7, 7)\n",
    "        recon_x = self.decoder(x_decoded)\n",
    "\n",
    "        return recon_x, mu, logvar, z\n",
    "\n",
    "    def sample(self, z, y):\n",
    "        y_onehot = F.one_hot(y, self.num_classes).float().to(z.device)\n",
    "        z_cat = torch.cat([z, y_onehot], dim=1)\n",
    "        x_decoded = self.fc_decode(z_cat)\n",
    "        x_decoded = x_decoded.view(z.size(0), 48, 7, 7)\n",
    "        samples = self.decoder(x_decoded)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03423541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_conditional_images(model, epoch, num_classes=5, latent_dim=50, device=DEVICE):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_classes, latent_dim).to(device)\n",
    "        y = torch.arange(num_classes).to(device)\n",
    "        samples = model.sample(z, y).cpu()\n",
    "\n",
    "        _, axes = plt.subplots(1, num_classes, figsize=(num_classes*2, 2))\n",
    "        for i in range(num_classes):\n",
    "            ax = axes[i]\n",
    "            ax.imshow(samples[i].squeeze(), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(classes[i])\n",
    "\n",
    "        plt.suptitle(f'Sampled Images at Epoch {epoch}', y=1.05)\n",
    "        plt.subplots_adjust(top=0.8)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64faa8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots a UMAP projection to visualize the latent space of the model\n",
    "def plot_umap(z_all, y_all, epoch, class_names):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "        reducer = umap.UMAP(n_components=2, random_state=None)\n",
    "    z_2d = reducer.fit_transform(z_all)\n",
    "\n",
    "    # Get unique classes sorted to align with colorbar ticks\n",
    "    unique_classes = sorted(set(y_all))\n",
    "\n",
    "    # Create a dict mapping class index to name\n",
    "    class_labels = [class_names[c] for c in unique_classes]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=y_all, cmap='tab10', alpha=0.6)\n",
    "\n",
    "    # Setup colorbar with ticks and labels\n",
    "    cbar = plt.colorbar(scatter, ticks=unique_classes)\n",
    "    cbar.ax.set_yticklabels(class_labels)\n",
    "\n",
    "    plt.title(f't-SNE Projection of Latent Space at Epoch {epoch}')\n",
    "    plt.xlabel(\"t-SNE dim 1\")\n",
    "    plt.ylabel(\"t-SNE dim 2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da84126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation loop for the CVAE model\n",
    "\n",
    "kl = []\n",
    "recon = []\n",
    "loss = []\n",
    "\n",
    "# Beta parameter for KL divergence weighting\n",
    "\"\"\"\n",
    "beta = 5 caused KL Divergence to collapse\n",
    "beta = 2 training plateaued after 10 epochs\n",
    "\"\"\"\n",
    "beta = 1.5\n",
    "\n",
    "model = CVAE().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Params for early stopping\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Create directories for saving weights if they don't exist\n",
    "os.makedirs(os.path.dirname(f'../weights/generative/cvae.pt'), exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    z_all = []\n",
    "    y_all = []\n",
    "\n",
    "    for i, (x, labels) in enumerate(TRAIN_LOADER):\n",
    "        x = x.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        x_reconst, mu, log_var, z = model(x,labels)\n",
    "        reconst_loss, kl_div, train_loss = vae_loss(x_reconst, x, mu, log_var, beta)\n",
    "\n",
    "        recon.append(reconst_loss.item()/len(x))\n",
    "        kl.append(kl_div.item()/len(x))\n",
    "        loss.append(train_loss.item()/len(x))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        z_all.append(z.detach().cpu())\n",
    "        y_all.append(labels.detach().cpu())\n",
    "\n",
    "    # Early stopping logic\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model, f'../weights/generative/cvae.pt')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Learning Rate {optimizer.param_groups[0]['lr']:.4f}\\n\"\n",
    "        f\"Loss: {train_loss.item()/len(x):.4f} | \"\n",
    "        f\"Reconstruction Loss: {reconst_loss.item()/len(x):.4f} | \"\n",
    "        f\"KL Divergence: {kl_div.item()/len(x):.4f}\")\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "        # Visualize the latent space using UMAP\n",
    "        z_all = torch.cat(z_all)\n",
    "        y_all = torch.cat(y_all)\n",
    "        plot_umap(z_all.numpy(), y_all.numpy(), epoch + 1, classes)\n",
    "\n",
    "        sample_conditional_images(model, epoch + 1, model.num_classes, model.latent_dim, DEVICE)\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3efab",
   "metadata": {},
   "source": [
    "## Conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4604263",
   "metadata": {},
   "source": [
    "## Conditional Diffusion Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
