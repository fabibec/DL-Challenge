{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929afcb6",
   "metadata": {},
   "source": [
    "# Generative Model\n",
    "\n",
    "This notebook deals with finding a good generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0f9e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.1982])\n",
      "Std: tensor([0.3426])\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import import_ipynb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils import * # type: ignore\n",
    "import os\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import warnings\n",
    "from torchvision.transforms import v2\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "354591b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 120\n",
    "PATIENCE = 50\n",
    "\n",
    "NOISE_DIM = 100\n",
    "GAN_LATENT_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a13d3e",
   "metadata": {},
   "source": [
    "## Conditional Variatonal Autoencoder\n",
    "\n",
    "First of all I want to start with a CNN-based variatonal Autoencoder. This way I can apply the knowledge that I gained while building the Classifier Model earlier.\n",
    "\n",
    "In order to put more focus on the class label I did embed the labels before passing them into the encoder, as well concatonating the image label to the decoder input.\n",
    "\n",
    "Moreover I did try to use KL annealing where I linearly increase the weight of the KL Divergence in the loss function to focus on proper reconstruction first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb7b379-8ae9-4fcf-b86c-4bb6e284e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No normalization needed for CVAE\n",
    "vae_train_transforms = v2.Compose([\n",
    "    # Include data augmentation, but exclude normalization\n",
    "    *train_transforms.transforms[:-1],\n",
    "])\n",
    "\n",
    "vae_test_transforms = v2.Compose([\n",
    "    # Include data augmentation, but exclude normalization\n",
    "    *test_transforms.transforms[:-1],\n",
    "])\n",
    "\n",
    "# Create Train and Test Datasets\n",
    "train_data = QuickDrawDataset('../dataset/train.csv', '../dataset/images', vae_train_transforms)\n",
    "test_data = QuickDrawDataset('../dataset/test.csv', '../dataset/images', vae_test_transforms)\n",
    "\n",
    "# Data loaders\n",
    "TRAIN_LOADER = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=6)\n",
    "TEST_LOADER = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce2e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE loss function - combines reconstruction loss and KL divergence\n",
    "def vae_loss(recon_x, x, mu, logvar, class_logits, labels, beta=1.0, gamma=1.0):\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    CLASS = F.cross_entropy(class_logits, labels, reduction='sum')\n",
    "    return MSE, KLD, CLASS, (MSE + beta * KLD + gamma * CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a45322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conditional Variational Autoencoder (CVAE) model\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=150, num_classes=len(classes)):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # label embedding to inject the information into the encoder\n",
    "        self.label_embedding = nn.Embedding(num_classes, 8) \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1 + 8, 128, kernel_size=3, padding=1),  # 1 channel image + 1 channel label map\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "        # Latent mappings\n",
    "        self.fc_input_dim = 256 * 7 * 7\n",
    "        self.fc_mu = nn.Linear(self.fc_input_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.fc_input_dim, latent_dim)\n",
    "\n",
    "        # Classifier head on latent space\n",
    "        self.classifier = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim + num_classes, 256 * 7 * 7)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # Reparameterization trick to sample from the latent space\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Embed image label \n",
    "        label_embed = self.label_embedding(y)\n",
    "        label_map = label_embed.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, 28, 28)\n",
    "        x_cat = torch.cat([x, label_map], dim=1) \n",
    "\n",
    "        # Encode\n",
    "        x_encoded = self.encoder(x_cat)\n",
    "\n",
    "        # mu + log_var\n",
    "        x_flat = x_encoded.view(batch_size, -1)\n",
    "        mu = self.fc_mu(x_flat)\n",
    "        logvar = self.fc_logvar(x_flat)\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Class logits from latent vector z\n",
    "        class_logits = self.classifier(z)\n",
    "\n",
    "        # Decode + add class label\n",
    "        y_onehot = F.one_hot(y, self.num_classes).float().to(x.device)\n",
    "        z_cat = torch.cat([z, y_onehot], dim=1)\n",
    "        x_decoded = self.fc_decode(z_cat)\n",
    "        x_decoded = x_decoded.view(batch_size, 256, 7, 7)\n",
    "        recon_x = self.decoder(x_decoded)\n",
    "\n",
    "        return recon_x, mu, logvar, z, class_logits\n",
    "\n",
    "    def sample(self, z, y):\n",
    "        y_onehot = F.one_hot(y, self.num_classes).float().to(z.device)\n",
    "        z_cat = torch.cat([z, y_onehot], dim=1)\n",
    "        x_decoded = self.fc_decode(z_cat)\n",
    "        x_decoded = x_decoded.view(z.size(0), 256, 7, 7)\n",
    "        samples = self.decoder(x_decoded)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9e0e42-f3ba-4173-aad7-2ffe63bef230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid like anealing\n",
    "def kl_anneal_sigmoid(epoch, total_epochs, max_beta=1.0, k=0.1, x0=None):\n",
    "    if x0 is None:\n",
    "        # midpoint of annealing\n",
    "        x0 = total_epochs / 2  \n",
    "    beta = max_beta / (1 + math.exp(-k * (epoch - x0)))\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03423541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_conditional_images(model, epoch, num_classes=5, latent_dim=50, device=DEVICE):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_classes, latent_dim).to(device)\n",
    "        y = torch.arange(num_classes).to(device)\n",
    "        samples = model.sample(z, y).cpu()\n",
    "\n",
    "        _, axes = plt.subplots(1, num_classes, figsize=(num_classes*2, 2))\n",
    "        for i in range(num_classes):\n",
    "            ax = axes[i]\n",
    "            ax.imshow(samples[i].squeeze(), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(classes[i])\n",
    "\n",
    "        plt.suptitle(f'Sampled Images at Epoch {epoch}', y=1.05)\n",
    "        plt.subplots_adjust(top=0.8)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64faa8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots a UMAP projection to visualize the latent space of the model\n",
    "def plot_umap(z_all, y_all, epoch, class_names):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "        reducer = umap.UMAP(n_components=2, random_state=None)\n",
    "    z_2d = reducer.fit_transform(z_all)\n",
    "\n",
    "    # Get unique classes sorted to align with colorbar ticks\n",
    "    unique_classes = sorted(set(y_all))\n",
    "\n",
    "    # Create a dict mapping class index to name\n",
    "    class_labels = [class_names[c] for c in unique_classes]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=y_all, cmap='tab10', alpha=0.6)\n",
    "\n",
    "    # Setup colorbar with ticks and labels\n",
    "    cbar = plt.colorbar(scatter, ticks=unique_classes)\n",
    "    cbar.ax.set_yticklabels(class_labels)\n",
    "\n",
    "    plt.title(f'UMAP Projection of Latent Space at Epoch {epoch}')\n",
    "    plt.xlabel(\"UMAP dim 1\")\n",
    "    plt.ylabel(\"UMAP dim 2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4da84126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Learning Rate 0.0010\n",
      "Loss: 296.6012 | Reconstruction Loss: 75.3498 | Beta: 0.1517 | KL Divergence: 1454.0845 | Classification Loss: 1.2860\n",
      "Epoch 2 | Learning Rate 0.0010\n",
      "Loss: nan | Reconstruction Loss: nan | Beta: 0.1663 | KL Divergence: nan | Classification Loss: nan\n",
      "Epoch 3 | Learning Rate 0.0010\n",
      "Loss: nan | Reconstruction Loss: nan | Beta: 0.1822 | KL Divergence: nan | Classification Loss: nan\n",
      "Epoch 4 | Learning Rate 0.0010\n",
      "Loss: nan | Reconstruction Loss: nan | Beta: 0.1995 | KL Divergence: nan | Classification Loss: nan\n",
      "Epoch 5 | Learning Rate 0.0010\n",
      "Loss: nan | Reconstruction Loss: nan | Beta: 0.2182 | KL Divergence: nan | Classification Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _afterFork at 0x7f1c4923b060>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.13/logging/__init__.py\", line 245, in _afterFork\n",
      "    def _afterFork():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 156653, 156654, 156655, 156656, 156657) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/dist-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1283\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/multiprocessing/queues.py:112\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m z_all = []\n\u001b[32m     26\u001b[39m y_all = []\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_LOADER\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/dist-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/dist-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/dist-packages/torch/utils/data/dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/dist-packages/torch/utils/data/dataloader.py:1297\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1296\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1298\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1299\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 156653, 156654, 156655, 156656, 156657) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Training and evaluation loop for the CVAE model\n",
    "kl = []\n",
    "recon = []\n",
    "claz = []\n",
    "loss = []\n",
    "\n",
    "# Beta parameter for KL divergence weighting\n",
    "beta = 2\n",
    "# Gamma parameter for classification loss weighting\n",
    "gamma = 0.5\n",
    "\n",
    "model = CVAE().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Params for early stopping\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Create directories for saving weights if they don't exist\n",
    "os.makedirs(os.path.dirname(f'../weights/generative/cvae.pt'), exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    z_all = []\n",
    "    y_all = []\n",
    "\n",
    "    for i, (x, labels) in enumerate(TRAIN_LOADER):\n",
    "        x = x.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        x_reconst, mu, log_var, z, class_logits = model(x,labels)\n",
    "        # Set the midpoint quite early\n",
    "        current_beta = kl_anneal_sigmoid(epoch, NUM_EPOCHS, beta, x0=25)\n",
    "        reconst_loss, kl_div, class_loss, train_loss = vae_loss(x_reconst, x, mu, log_var, class_logits, labels, current_beta, gamma)\n",
    "\n",
    "        recon.append(reconst_loss.item()/len(x))\n",
    "        kl.append(kl_div.item()/len(x))\n",
    "        loss.append(train_loss.item()/len(x))\n",
    "        claz.append(class_loss.item()/len(x))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        z_all.append(z.detach().cpu())\n",
    "        y_all.append(labels.detach().cpu())\n",
    "\n",
    "    # Early stopping logic\n",
    "    if train_loss.item()/len(x) < best_loss:\n",
    "        best_loss = train_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model, f'../weights/generative/cvae.pt')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Learning Rate {optimizer.param_groups[0]['lr']:.4f}\\n\"\n",
    "        f\"Loss: {train_loss.item()/len(x):.4f} | \"\n",
    "        f\"Reconstruction Loss: {reconst_loss.item()/len(x):.4f} | \"\n",
    "        f\"Beta: {current_beta:.4f} | \"\n",
    "        f\"KL Divergence: {kl_div.item()/len(x):.4f} | \"\n",
    "        f\"Classification Loss: {class_loss.item()/len(x):.4f}\")\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            # Visualize the latent space using UMAP\n",
    "            z_all = torch.cat(z_all)\n",
    "            y_all = torch.cat(y_all)\n",
    "            plot_umap(z_all.numpy(), y_all.numpy(), epoch + 1, classes)\n",
    "        sample_conditional_images(model, epoch + 1, model.num_classes, model.latent_dim, DEVICE)\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b112b5-6cbb-416b-9818-be76caf52999",
   "metadata": {},
   "source": [
    "## Conditional GAN\n",
    "\n",
    "Based on this [paper](https://arxiv.org/pdf/1511.06434) I want to build a conditional version that is able to create images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cca8d9-57e4-44a3-94a9-510cb1067913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GAN expected images with values from [-1, 1] so we need different normalizations\n",
    "cgan_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Add data augmentations during training\n",
    "cgan_train_transforms = v2.Compose([\n",
    "    *train_transforms.transforms[:-3],\n",
    "    *cgan_transforms.transforms\n",
    "])\n",
    "\n",
    "# cGAN Dataset and Dataloader\n",
    "cgan_train_data = QuickDrawDataset('../dataset/train.csv', '../dataset/images', cgan_train_transforms)\n",
    "cgan_test_data = QuickDrawDataset('../dataset/test.csv', '../dataset/images', cgan_transforms)\n",
    "\n",
    "cgan_train_loader = DataLoader(cgan_train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=6, pin_memory=True)\n",
    "cgan_test_loader = DataLoader(cgan_test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=6, pin_memory=True)\n",
    "    \n",
    "# The Generator\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, num_classes=5, embedding_dim=100):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "\n",
    "        # Conditional Part: Embedd label and add concat with input\n",
    "        self.label_embed = nn.Embedding(num_classes, embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim * 2, 512 * 7 * 7),  # was 256*7*7\n",
    "            nn.BatchNorm1d(512 * 7 * 7),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            # Upsample 7x7 -> 14x14, reduce channels from 512 to 256\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),  \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Upsample 14x14 -> 28x28, reduce channels from 256 to 128\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Final conv to get single channel image 28x28, no stride/padding change\n",
    "            nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.Tanh()  # normalize to [-1,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        label_embedding = self.label_embed(labels)\n",
    "        x = torch.cat([noise, label_embedding], dim=1)\n",
    "        x = self.fc(x).view(-1, 512, 7, 7)\n",
    "        return self.net(x)\n",
    "\n",
    "# The Discriminator\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, num_classes=5, embedding_dim=100):\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "\n",
    "        # Conditional Part: Embedd label and add concat with input\n",
    "        self.label_embed = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.label_fc = nn.Linear(embedding_dim, 28 * 28)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1, bias=False),  # 28x28 -> 14x14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),  # 14x14 -> 7x7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "            nn.Conv2d(128, 1, kernel_size=7, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_embedding = self.label_embed(labels)\n",
    "        label_img = self.label_fc(label_embedding).view(-1, 1, 28, 28)\n",
    "        x = torch.cat([img, label_img], dim=1)  # concat on channel dim\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2bfe3-bdde-484b-8e56-50848f8a60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(generator, noise, image_labels, epoch):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_images = generator(noise, image_labels) \n",
    "        generated_images = generated_images * 0.5 + 0.5  # Unnormalize from [-1, 1] to [0, 1]\n",
    "\n",
    "    # Plot\n",
    "    # Get model predictions -> logits\n",
    "    _, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        img = generated_images[i]\n",
    "        img = img.squeeze(0).cpu()\n",
    "\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f\"Class {classes[image_labels[i].item()]}\")\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.suptitle(f'Sampled Images at Epoch {epoch}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "406c8ec5-6496-4afe-b75e-fe1e108cf012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss_D: 0.3681 | Loss_G: 0.3910\n",
      "Epoch 2 | Loss_D: 0.3773 | Loss_G: 0.3603\n",
      "Epoch 3 | Loss_D: 0.3688 | Loss_G: 0.3965\n",
      "Epoch 4 | Loss_D: 0.3728 | Loss_G: 0.3880\n",
      "Epoch 5 | Loss_D: 0.3767 | Loss_G: 0.3885\n",
      "Epoch 6 | Loss_D: 0.3852 | Loss_G: 0.3556\n",
      "Epoch 7 | Loss_D: 0.3713 | Loss_G: 0.3998\n",
      "Epoch 8 | Loss_D: 0.3829 | Loss_G: 0.3848\n",
      "Epoch 9 | Loss_D: 0.3826 | Loss_G: 0.3721\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, NUM_EPOCHS):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, (real_imgs, image_labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cgan_train_loader):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         real_imgs = \u001b[43mreal_imgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m         image_labels = image_labels.to(DEVICE)\n\u001b[32m     31\u001b[39m         \u001b[38;5;66;03m# Add a bit of gaussian noice to images in order to prevent the discriminator from becoming to good\u001b[39;00m\n\u001b[32m     32\u001b[39m         \u001b[38;5;66;03m# Decay the added noise over time\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Init models\n",
    "G = ConditionalGenerator(noise_dim=NOISE_DIM, num_classes=len(classes)).to(DEVICE)\n",
    "D = ConditionalDiscriminator(num_classes=len(classes)).to(DEVICE)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "# Pregenerated fixed nose and fixed labels to check if generation improves\n",
    "fixed_noise = torch.randn(25, NOISE_DIM, device=DEVICE)\n",
    "fixed_image_labels = torch.arange(0, len(classes)).repeat(5).to(DEVICE)\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "for epoch in range(0, NUM_EPOCHS):\n",
    "    for i, (real_imgs, image_labels) in enumerate(cgan_train_loader):\n",
    "        real_imgs = real_imgs.to(DEVICE)\n",
    "        image_labels = image_labels.to(DEVICE)\n",
    "\n",
    "        # Add a bit of gaussian noice to images in order to prevent the discriminator from becoming to good\n",
    "        # Decay the added noise over time\n",
    "        noise_std = max(0.1 * (1.0 - epoch / NUM_EPOCHS), 0.01)\n",
    "        instance_noise = torch.randn_like(real_imgs) * noise_std\n",
    "        noisy_real_images = real_imgs + instance_noise\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Generate fake images\n",
    "        z = torch.randn(BATCH_SIZE, GAN_LATENT_DIM, device=DEVICE)\n",
    "        fake_image_labels = image_labels\n",
    "        fake_images = G(z, fake_image_labels)\n",
    "\n",
    "        # Add instance noise to fake images\n",
    "        noisy_fake_images = fake_images + torch.randn_like(fake_images) * noise_std\n",
    "\n",
    "        # Fake labels in [0.0, 0.25]\n",
    "        fake_d_labels = torch.empty(BATCH_SIZE, device=DEVICE).uniform_(0.0, 0.25)\n",
    "        \n",
    "        # Generate real labels in [0.75, 1.0] - prevent discriminator  overconfidence\n",
    "        real_d_labels = torch.empty(BATCH_SIZE, device=DEVICE).uniform_(0.75, 1.0)\n",
    "\n",
    "        # Clamp noisy images to keep training stable\n",
    "        noisy_real_images = torch.clamp(noisy_real_images, -1, 1)\n",
    "        noisy_fake_images = torch.clamp(noisy_fake_images, -1, 1)\n",
    "\n",
    "        # Shuffle real and fake images to prevent ordering bias\n",
    "        all_images = torch.cat([noisy_real_images, noisy_fake_images], dim=0)\n",
    "        all_image_labels = torch.cat([image_labels, fake_image_labels], dim=0)\n",
    "        all_d_labels = torch.cat([real_d_labels, fake_d_labels], dim=0)\n",
    "\n",
    "        perm = torch.randperm(all_images.size(0))\n",
    "        shuffled_images = all_images[perm]\n",
    "        shuffled_image_labels = all_image_labels[perm]\n",
    "        shuffled_d_labels = all_d_labels[perm]\n",
    "\n",
    "        outputs = D(shuffled_images, shuffled_image_labels)\n",
    "        loss_D = criterion(outputs.view(-1), shuffled_d_labels)\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate fake images again\n",
    "        fake_images = G(z, fake_image_labels)\n",
    "\n",
    "        # Get discriminator's prediction on these fake images with their labels\n",
    "        outputs = D(fake_images, fake_image_labels)\n",
    "\n",
    "        # Generator wants the discriminator to believe these are real\n",
    "        gen_targets = torch.empty(BATCH_SIZE, device=DEVICE).uniform_(0.75, 1.0)\n",
    "\n",
    "        # Compute generator loss \n",
    "        loss_G = criterion(outputs.view(-1), gen_targets)\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss_D: {loss_D.item():.4f} | Loss_G: {loss_G.item():.4f}\")\n",
    "\n",
    "    # Save sample images\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        G.eval()\n",
    "        sample_images(G, fixed_noise, fixed_image_labels, epoch+1)\n",
    "        G.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82838d-362b-4790-9d93-f47d3ec823ff",
   "metadata": {},
   "source": [
    "## Non-conditional GAN\n",
    "\n",
    "Maybe training the generator to produce output for every class is less efficient then just training five smaller models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb5a9c-6f1e-412f-98c7-ede510af9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4604263",
   "metadata": {},
   "source": [
    "## Conditional Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23a4e0-4434-44da-902d-7841557fea28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
