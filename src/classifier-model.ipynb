{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b67b152",
   "metadata": {},
   "source": [
    "# Classifier model\n",
    "\n",
    "This notebook deals with finding a classifier model to classify a given image into a correct class. Therefor I will evaluate different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8006fd",
   "metadata": {},
   "source": [
    "## Setup and general preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef7abbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import import_ipynb\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils import DEVICE, classes, train_data, test_data, BaseModule # type: ignore\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 30\n",
    "PATIENCE = 3\n",
    "\n",
    "# Data loaders\n",
    "TRAIN_LOADER = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=6)\n",
    "TEST_LOADER = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=6)\n",
    "\n",
    "# Loss functions\n",
    "TRAIN_CRITERION = nn.CrossEntropyLoss() # CrossEntropyLoss combines log-softmax + NLLLoss\n",
    "TEST_CRITERION = nn.CrossEntropyLoss(reduction='sum') # For test function\n",
    "\n",
    "# LR scheduler lambda function\n",
    "def lr_lambda(epoch):\n",
    "    return 0.95 ** (epoch - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9651a6",
   "metadata": {},
   "source": [
    "### Training, Validation and Evaluation Workflow\n",
    "\n",
    "In order to compare different models to each other faster, I wrote central train, test and evaluation functions. This way I'm able to just create a bunch of different models and compare them easily. \n",
    "\n",
    "For evaluation I choose the following metrics:\n",
    "\n",
    "| Metric               | Formula                               | Description                                                                                    | \n",
    "|----------------------|---------------------------------------|------------------------------------------------------------------------------------------------|\n",
    "| `Accuracy`           | Overall correct predictions} / total  | How well the model performs in general.                                                        |\n",
    "| `Precision`          | TP / (TP + FP)                        | How many of the images predicted to be in a class are actually that class.                     |\n",
    "| `Recall`             | TP / (TP + FN)                        | How many of the images in a class did the model classify correctly.                            |\n",
    "| `F1-score`           | Harmonic mean of precision and recall | Balances precision and recall, shows how well the model doing on each class overall            |                  \n",
    "| `Confusion Matrix`   | -                                     | Shows precisely what image as been classified as which class in comparison to the ground truth |\n",
    "\n",
    "By employing other metrics than just the accuracy I'm able to get a more detailed and classwise view of model. Even though the classes are balanced maybe the model can't distinguish between to particular classes. This could then fixed by penalizing the false classifications more, or oversampling these images with some additional transforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e3497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Epoch train function\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, targets in dataloader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "\n",
    "        # We use CrossEntropyLoss which combines softmax and NLLLoss\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = logits.max(1)\n",
    "        correct += preds.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# One Epoch test function\n",
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            _, preds = logits.max(1)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Generate classification report, which includes precision, recall, and F1-score -> take the macro averaged ones\n",
    "    report = classification_report(all_targets, all_preds, zero_division=0.0, output_dict=True)\n",
    "    precision = report['macro avg']['precision']\n",
    "    recall = report['macro avg']['recall']\n",
    "    f1 = report['macro avg']['f1-score']\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "# Training loop for one model\n",
    "def run_training_for_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    model_name='model'\n",
    "):\n",
    "    # Save the metrics history for plotting later\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_prec': [],\n",
    "        'val_rec': [],\n",
    "        'val_f1': [],\n",
    "    }\n",
    "\n",
    "    # Params for early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(f\"Start training {model_name}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1 = test(model, test_loader, criterion, device)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1} for model {model_name}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save metrics\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_prec'].append(val_prec)\n",
    "        history['val_rec'].append(val_rec)\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Learning Rate {optimizer.param_groups[0]['lr']:.4f}\\n\"\n",
    "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {train_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    print(f\"Training model {model_name} complete!\")\n",
    "\n",
    "    # Return history for evaluation and plotting\n",
    "    return history\n",
    "\n",
    "# Plotting metric history for all models\n",
    "def plot_training_history(results):\n",
    "\n",
    "    # Use a different color cycle for each model\n",
    "    colors = plt.colormaps['tab10'].colors\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "    color_cycle = itertools.cycle(colors)\n",
    "    style_cycle = itertools.cycle(line_styles)\n",
    "\n",
    "    num_models = len(results)\n",
    "    model_names = list(results.keys())\n",
    "\n",
    "    # Reset for consistent color/style per model\n",
    "    model_styles = {}\n",
    "    for name in model_names:\n",
    "        model_styles[name] = {\n",
    "            \"color\": next(color_cycle),\n",
    "            \"style\": next(style_cycle)\n",
    "        }\n",
    "\n",
    "    # Create plot grid: 4 rows x 2 columns\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(16, 20))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Metrics and their subplot indices\n",
    "    metric_info = {\n",
    "        'train_loss': (0, 'Training Loss'),\n",
    "        'train_acc':  (1, 'Training Accuracy'),\n",
    "        'val_loss':   (2, 'Validation Loss'),\n",
    "        'val_acc':    (3, 'Validation Accuracy'),\n",
    "        'val_prec':   (4, 'Validation Precision'),\n",
    "        'val_rec':    (5, 'Validation Recall'),\n",
    "        'val_f1':     (6, 'Validation F1 Score'),\n",
    "    }\n",
    "\n",
    "    for metric, (idx, title) in metric_info.items():\n",
    "        ax = axs[idx]\n",
    "        for name, history in results.items():\n",
    "            color = model_styles[name][\"color\"]\n",
    "            style = model_styles[name][\"style\"]\n",
    "            ax.plot(history[metric], label=name, color=color, linestyle=style)\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    # Hide the unused 8th subplot\n",
    "    axs[7].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: plot confusion matrix\n",
    "\n",
    "# Main evaluation function to run the training and evaluation\n",
    "def evaluate_models(models):\n",
    "\n",
    "    results = {name: None for name in models.keys()}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "        history = run_training_for_model(\n",
    "            model=model,\n",
    "            train_loader=TRAIN_LOADER,\n",
    "            test_loader=TEST_LOADER,\n",
    "            criterion=TRAIN_CRITERION,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=DEVICE,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            model_name=name\n",
    "        )\n",
    "\n",
    "        results[name] = history\n",
    "\n",
    "    # Plot the training history for all models\n",
    "    plot_training_history(results)\n",
    "\n",
    "# Helper function to print model summary\n",
    "def print_model_summary(models_dict):\n",
    "    for model in models_dict.values():\n",
    "        summary(model, input_size=(1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18fd30",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "Since the dataset is very similar to the MNIST dataset I first want to explore training a CNN from scratch to get a baseline for the performance.\n",
    "\n",
    "Moreover I found this kaggle notebook that evaluated some choices regarding the CNN architecture. In the following I will try to replicate these experiments on this dataset.\n",
    "\n",
    "Reference:\n",
    " - [Kaggle Notebook](https://www.kaggle.com/code/cdeotte/how-to-choose-cnn-architecture-mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2c6c8",
   "metadata": {},
   "source": [
    "#### Test 1: Amount of Convolution + Pooling Layers\n",
    "\n",
    "Based on the referenced notebook I want to check how many of these layer combinations produce the best output. More layers would result in very small images that will likely not contain any valuable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a4414b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 28, 28]             624\n",
      "         LeakyReLU-2           [-1, 24, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 24, 14, 14]               0\n",
      "           Flatten-4                 [-1, 4704]               0\n",
      "            Linear-5                  [-1, 256]       1,204,480\n",
      "         LeakyReLU-6                  [-1, 256]               0\n",
      "            Linear-7                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 1,206,389\n",
      "Trainable params: 1,206,389\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 4.60\n",
      "Estimated Total Size (MB): 4.97\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 28, 28]             624\n",
      "         LeakyReLU-2           [-1, 24, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 24, 14, 14]               0\n",
      "            Conv2d-4           [-1, 48, 14, 14]          28,848\n",
      "         LeakyReLU-5           [-1, 48, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 48, 7, 7]               0\n",
      "           Flatten-7                 [-1, 2352]               0\n",
      "            Linear-8                  [-1, 256]         602,368\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 633,125\n",
      "Trainable params: 633,125\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.51\n",
      "Params size (MB): 2.42\n",
      "Estimated Total Size (MB): 2.92\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 28, 28]             624\n",
      "         LeakyReLU-2           [-1, 24, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 24, 14, 14]               0\n",
      "            Conv2d-4           [-1, 48, 14, 14]          28,848\n",
      "         LeakyReLU-5           [-1, 48, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 48, 7, 7]               0\n",
      "            Conv2d-7             [-1, 64, 7, 7]          76,864\n",
      "         LeakyReLU-8             [-1, 64, 7, 7]               0\n",
      "         MaxPool2d-9             [-1, 64, 3, 3]               0\n",
      "          Flatten-10                  [-1, 576]               0\n",
      "           Linear-11                  [-1, 256]         147,712\n",
      "        LeakyReLU-12                  [-1, 256]               0\n",
      "           Linear-13                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 255,333\n",
      "Trainable params: 255,333\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.55\n",
      "Params size (MB): 0.97\n",
      "Estimated Total Size (MB): 1.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build the three CNN models\n",
    "nets = 3\n",
    "cnn_conv_layer_models = {f'CNN_{n + 1}_Conv': BaseModule() for n in range(nets)}\n",
    "\n",
    "for i, model in enumerate(cnn_conv_layer_models.values()):\n",
    "\n",
    "    # First Convolutional Layer (in every model)\n",
    "    model.layers.append(nn.Conv2d(1, 24, kernel_size=5, padding='same'))\n",
    "    model.layers.append(nn.LeakyReLU())\n",
    "    model.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    if i > 0:\n",
    "        # Second Convolutional Layer (in every model except the first)\n",
    "        model.layers.append(nn.Conv2d(24, 48, kernel_size=5, padding='same'))\n",
    "        model.layers.append(nn.LeakyReLU())\n",
    "        model.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Third Convolutional Layer (only in the third model)\n",
    "        if i == 2:\n",
    "            model.layers.append(nn.Conv2d(48, 64, kernel_size=5, padding='same'))\n",
    "            model.layers.append(nn.LeakyReLU())\n",
    "            model.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    # Output sizes after convolutional layers\n",
    "    # 28x28 -> 14x14 -> 7x7 -> 3x3\n",
    "    # 24*14*14 -> 48*7*7 -> 64*3*3\n",
    "    conv_out = [24*14*14, 48*7*7, 64*3*3]\n",
    "\n",
    "    model.layers.append(nn.Flatten())\n",
    "    model.layers.append(nn.Linear(conv_out[i], 256))\n",
    "    model.layers.append(nn.LeakyReLU())\n",
    "    model.layers.append(nn.Linear(256, 5))\n",
    "\n",
    "print_model_summary(cnn_conv_layer_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "409eea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training CNN_1_Conv\n",
      "Epoch 1 | Learning Rate 0.0009\n",
      "Train Loss: 0.4957 | Train Acc: 0.8324 | Val Acc: 0.8324 | Val F1: 0.9156\n",
      "Epoch 2 | Learning Rate 0.0009\n",
      "Train Loss: 0.3072 | Train Acc: 0.9002 | Val Acc: 0.9002 | Val F1: 0.9315\n",
      "Epoch 3 | Learning Rate 0.0009\n",
      "Train Loss: 0.2605 | Train Acc: 0.9143 | Val Acc: 0.9143 | Val F1: 0.9349\n",
      "Epoch 4 | Learning Rate 0.0008\n",
      "Train Loss: 0.2322 | Train Acc: 0.9233 | Val Acc: 0.9233 | Val F1: 0.9398\n",
      "Epoch 5 | Learning Rate 0.0008\n",
      "Train Loss: 0.2153 | Train Acc: 0.9293 | Val Acc: 0.9293 | Val F1: 0.9427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate the number of convolutional layers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_conv_layer_models\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 194\u001b[39m, in \u001b[36mevaluate_models\u001b[39m\u001b[34m(models)\u001b[39m\n\u001b[32m    191\u001b[39m     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\u001b[32m    192\u001b[39m     scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     history = \u001b[43mrun_training_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_LOADER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEST_LOADER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_CRITERION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     results[name] = history\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Plot the training history for all models\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mrun_training_for_model\u001b[39m\u001b[34m(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs, model_name)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStart training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     train_loss, train_acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     val_loss, val_acc, val_prec, val_rec, val_f1 = test(model, test_loader, criterion, device)\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Early stopping logic\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# We use CrossEntropyLoss which combines softmax and NLLLoss\u001b[39;00m\n\u001b[32m     15\u001b[39m loss = criterion(logits, targets)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Update running loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Evaluate the number of convolutional layers\n",
    "evaluate_models(cnn_conv_layer_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503afdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 28, 28]             208\n",
      "         LeakyReLU-2            [-1, 8, 28, 28]               0\n",
      "         MaxPool2d-3            [-1, 8, 14, 14]               0\n",
      "            Conv2d-4           [-1, 16, 14, 14]           3,216\n",
      "         LeakyReLU-5           [-1, 16, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 16, 7, 7]               0\n",
      "           Flatten-7                  [-1, 784]               0\n",
      "            Linear-8                  [-1, 256]         200,960\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 205,669\n",
      "Trainable params: 205,669\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.17\n",
      "Params size (MB): 0.78\n",
      "Estimated Total Size (MB): 0.96\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "         LeakyReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]          12,832\n",
      "         LeakyReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "           Flatten-7                 [-1, 1568]               0\n",
      "            Linear-8                  [-1, 256]         401,664\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 416,197\n",
      "Trainable params: 416,197\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.34\n",
      "Params size (MB): 1.59\n",
      "Estimated Total Size (MB): 1.93\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 28, 28]             624\n",
      "         LeakyReLU-2           [-1, 24, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 24, 14, 14]               0\n",
      "            Conv2d-4           [-1, 48, 14, 14]          28,848\n",
      "         LeakyReLU-5           [-1, 48, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 48, 7, 7]               0\n",
      "           Flatten-7                 [-1, 2352]               0\n",
      "            Linear-8                  [-1, 256]         602,368\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 633,125\n",
      "Trainable params: 633,125\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.51\n",
      "Params size (MB): 2.42\n",
      "Estimated Total Size (MB): 2.92\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             832\n",
      "         LeakyReLU-2           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
      "            Conv2d-4           [-1, 64, 14, 14]          51,264\n",
      "         LeakyReLU-5           [-1, 64, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 64, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3136]               0\n",
      "            Linear-8                  [-1, 256]         803,072\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 856,453\n",
      "Trainable params: 856,453\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.67\n",
      "Params size (MB): 3.27\n",
      "Estimated Total Size (MB): 3.94\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 40, 28, 28]           1,040\n",
      "         LeakyReLU-2           [-1, 40, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 40, 14, 14]               0\n",
      "            Conv2d-4           [-1, 80, 14, 14]          80,080\n",
      "         LeakyReLU-5           [-1, 80, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 80, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3920]               0\n",
      "            Linear-8                  [-1, 256]       1,003,776\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 1,086,181\n",
      "Trainable params: 1,086,181\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.84\n",
      "Params size (MB): 4.14\n",
      "Estimated Total Size (MB): 4.99\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 48, 28, 28]           1,248\n",
      "         LeakyReLU-2           [-1, 48, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 48, 14, 14]               0\n",
      "            Conv2d-4           [-1, 96, 14, 14]         115,296\n",
      "         LeakyReLU-5           [-1, 96, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 96, 7, 7]               0\n",
      "           Flatten-7                 [-1, 4704]               0\n",
      "            Linear-8                  [-1, 256]       1,204,480\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 1,322,309\n",
      "Trainable params: 1,322,309\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.01\n",
      "Params size (MB): 5.04\n",
      "Estimated Total Size (MB): 6.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nets = 6\n",
    "cnn_num_kernels_models = [BaseModule() for _ in range(nets)]\n",
    "for i in range(nets):\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Conv2d(1, i*8+8, kernel_size=5, padding='same'))\n",
    "    cnn_num_kernels_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_kernels_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Conv2d(i*8+8, i*16+16, kernel_size=5, padding='same'))\n",
    "    cnn_num_kernels_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_kernels_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Flatten())\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Linear((i*16+16)*7*7, 256))\n",
    "    cnn_num_kernels_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Linear(256, 5))\n",
    "\n",
    "# Print models\n",
    "for i in range(nets):\n",
    "    summary(cnn_num_kernels_models[i], input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41236cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 8 and 16 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.333955\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.344451\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.098977\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.232188\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.220662\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.130773\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.267109\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.113791\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.269854\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.093394\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.152315\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.209864\n",
      "\n",
      "Test set: Average loss: 0.1552, Accuracy: 23685/25000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.308205\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.399003\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.205928\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.154390\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.236444\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.232380\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.193305\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.174637\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.165314\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.098730\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.211798\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.177017\n",
      "\n",
      "Test set: Average loss: 0.1500, Accuracy: 23746/25000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.087508\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.217780\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.144492\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.108917\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.120703\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.089090\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.082845\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.266603\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.149023\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.164099\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.084195\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.219911\n",
      "\n",
      "Test set: Average loss: 0.1468, Accuracy: 23830/25000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.207574\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.163471\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.181908\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.263677\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.102562\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.135236\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.037650\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.089728\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.246489\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.059160\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.134700\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.180462\n",
      "\n",
      "Test set: Average loss: 0.1417, Accuracy: 23829/25000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.165644\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.132120\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.183984\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.054330\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.167584\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.191885\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.119254\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.058699\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.209246\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.091368\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.197240\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.294336\n",
      "\n",
      "Test set: Average loss: 0.1441, Accuracy: 23806/25000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.151534\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.251693\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.318758\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.279291\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.164991\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.078796\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.135709\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.261902\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.058261\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.072398\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.225287\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.089774\n",
      "\n",
      "Test set: Average loss: 0.1429, Accuracy: 23828/25000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.170296\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.124595\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.160630\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.167571\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.092782\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.151620\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.148043\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.127491\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.135988\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.181135\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.066734\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.159083\n",
      "\n",
      "Test set: Average loss: 0.1361, Accuracy: 23859/25000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.078976\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.114087\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.177859\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.078379\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.227821\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.143388\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.249558\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.094018\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.321006\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.200644\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.200470\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.250143\n",
      "\n",
      "Test set: Average loss: 0.1433, Accuracy: 23851/25000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.135130\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.121038\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.031195\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.092741\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.192206\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.096763\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.093716\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.072064\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.022808\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.183738\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.144056\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.061048\n",
      "\n",
      "Test set: Average loss: 0.1368, Accuracy: 23881/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.085463\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.106245\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.252396\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.127964\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.039561\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.062052\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.066052\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.036236\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.309119\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.218100\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.081046\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.070081\n",
      "\n",
      "Test set: Average loss: 0.1349, Accuracy: 23917/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.163916\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.323503\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.089310\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.070442\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.092544\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.283420\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.178493\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.155971\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.144651\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.151739\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.262201\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.171850\n",
      "\n",
      "Test set: Average loss: 0.1370, Accuracy: 23888/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.212582\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.287183\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.091193\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.150844\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.042034\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.046944\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.143650\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.146631\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.086582\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.095720\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.156922\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.063231\n",
      "\n",
      "Test set: Average loss: 0.1312, Accuracy: 23946/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.184643\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.084735\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.048874\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.254733\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.177864\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.048778\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.120090\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.097496\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.134071\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.084002\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.137642\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.144489\n",
      "\n",
      "Test set: Average loss: 0.1307, Accuracy: 23938/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.123217\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.201438\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.069475\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.164084\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.080212\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.125573\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.143792\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.259593\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.260826\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.072840\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.113005\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.048880\n",
      "\n",
      "Test set: Average loss: 0.1323, Accuracy: 23910/25000 (96%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.149295\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.058519\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.197346\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.232054\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.191716\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.023679\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.057996\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.283589\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.161991\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.116619\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.153608\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.139731\n",
      "\n",
      "Test set: Average loss: 0.1296, Accuracy: 23947/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.144081\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.133690\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.139744\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.036710\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.133875\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.232994\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.177454\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.318568\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.090175\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.094679\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.027220\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.063837\n",
      "\n",
      "Test set: Average loss: 0.1321, Accuracy: 23943/25000 (96%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.150816\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.122451\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.028721\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.127981\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.241953\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.045705\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.067133\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.179816\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.229336\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.110989\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.085171\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.079105\n",
      "\n",
      "Test set: Average loss: 0.1281, Accuracy: 23933/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.078493\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.066006\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.199797\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.031890\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.118610\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.099817\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.112308\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.235012\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.168877\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.168540\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.135744\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.024670\n",
      "\n",
      "Test set: Average loss: 0.1279, Accuracy: 23980/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.108661\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.110904\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.043129\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.073658\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.080033\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.228351\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.070859\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.149975\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.163319\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.041222\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.078755\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.108200\n",
      "\n",
      "Test set: Average loss: 0.1281, Accuracy: 23975/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.144213\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.094141\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.201630\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.175212\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.109980\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.095178\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.223696\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.060148\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.266226\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.034557\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.068445\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.068549\n",
      "\n",
      "Test set: Average loss: 0.1301, Accuracy: 23943/25000 (96%)\n",
      "\n",
      "Training model with 16 and 32 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.254799\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.080431\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.029685\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.209175\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.069811\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.068772\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.201683\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.088097\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.093030\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.091311\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.087720\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.161325\n",
      "\n",
      "Test set: Average loss: 0.1299, Accuracy: 23919/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.167689\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.213643\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.165459\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.149016\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.162819\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.228655\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.228755\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.131897\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.119662\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.068808\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.264149\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.065477\n",
      "\n",
      "Test set: Average loss: 0.1355, Accuracy: 23925/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.039603\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.180502\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.171913\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.171453\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.359618\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.260225\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.118950\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.066617\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.118902\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.117335\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.232123\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.124574\n",
      "\n",
      "Test set: Average loss: 0.1323, Accuracy: 23951/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.139779\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.075239\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.144170\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.105706\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.049003\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.076510\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.204166\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.120016\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.101648\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.158649\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.111316\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.154502\n",
      "\n",
      "Test set: Average loss: 0.1303, Accuracy: 23964/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.116690\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.114921\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.107097\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.054581\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.286050\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.091979\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.063270\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.137498\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.495823\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.115970\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.054712\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.054834\n",
      "\n",
      "Test set: Average loss: 0.1272, Accuracy: 23924/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.137932\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.120279\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.038741\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.213415\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.106443\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.299545\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.154277\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.079220\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.140779\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.122885\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.140998\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.120080\n",
      "\n",
      "Test set: Average loss: 0.1218, Accuracy: 24006/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.130154\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.127197\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.100948\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.076655\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.093308\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.151603\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.086434\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.061263\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.132264\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.052390\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.057039\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.170614\n",
      "\n",
      "Test set: Average loss: 0.1309, Accuracy: 23958/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.084877\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.085686\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.111528\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.104802\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.024131\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.110804\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.124951\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.157204\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.094040\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.055111\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.205246\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.173539\n",
      "\n",
      "Test set: Average loss: 0.1251, Accuracy: 23993/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.286447\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.028401\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.158259\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.022113\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.096909\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.014838\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.059709\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.143194\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.100293\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.112451\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.097647\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.057879\n",
      "\n",
      "Test set: Average loss: 0.1225, Accuracy: 24020/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.128305\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.121586\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.192818\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.219819\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.161008\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.052272\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.036903\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.065321\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.118224\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.135699\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.096491\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.084325\n",
      "\n",
      "Test set: Average loss: 0.1220, Accuracy: 24023/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.058296\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.085352\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.042994\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.159410\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.037746\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.144676\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.096840\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.050933\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.041832\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.087011\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.100266\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.130467\n",
      "\n",
      "Test set: Average loss: 0.1251, Accuracy: 23999/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.097775\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.079980\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.027274\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.058802\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.070667\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.088731\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.154716\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.026565\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.031665\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.245646\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.074630\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.126159\n",
      "\n",
      "Test set: Average loss: 0.1170, Accuracy: 24070/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.123365\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.107042\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.034474\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.062620\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.096784\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.179606\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.067048\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.124941\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.063974\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.056302\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.107978\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.048954\n",
      "\n",
      "Test set: Average loss: 0.1176, Accuracy: 24034/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.081366\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.077099\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.073028\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.018187\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.207785\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.125393\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.271130\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.117772\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.095885\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.130177\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.119643\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.057736\n",
      "\n",
      "Test set: Average loss: 0.1186, Accuracy: 24055/25000 (96%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.032929\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.182938\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.130040\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.165130\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.060429\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.082875\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.067303\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.022743\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.078607\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.021577\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.042529\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.046930\n",
      "\n",
      "Test set: Average loss: 0.1194, Accuracy: 24050/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.112571\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.200747\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.027154\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.042198\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.064204\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.059792\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.066278\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.118229\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.154344\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.030210\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.175307\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.019164\n",
      "\n",
      "Test set: Average loss: 0.1225, Accuracy: 24042/25000 (96%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.073124\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.138784\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.065448\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.081478\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.284068\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.158661\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.018851\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.074684\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.186644\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.084865\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.079102\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.164263\n",
      "\n",
      "Test set: Average loss: 0.1186, Accuracy: 24051/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.090278\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.077388\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.111468\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.018161\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.028266\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.146465\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.101655\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.118849\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.171212\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.176815\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.090488\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.084201\n",
      "\n",
      "Test set: Average loss: 0.1194, Accuracy: 24058/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.047630\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.160799\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.105260\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.050428\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.080825\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.108745\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.058420\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.089105\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.109304\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.168278\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.050449\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.108318\n",
      "\n",
      "Test set: Average loss: 0.1168, Accuracy: 24087/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.159643\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.100357\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.145246\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.046407\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.089418\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.060134\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.165257\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.133830\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.010786\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.030480\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.095689\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.073547\n",
      "\n",
      "Test set: Average loss: 0.1226, Accuracy: 24045/25000 (96%)\n",
      "\n",
      "Training model with 24 and 48 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.142370\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.152594\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.097316\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.259115\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.107201\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.063849\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.129817\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.162869\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.095971\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.153145\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.155123\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.117100\n",
      "\n",
      "Test set: Average loss: 0.1268, Accuracy: 23966/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.072739\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.215588\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.068119\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.073014\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.180380\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.121357\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.241472\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.066531\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.277672\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.074038\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.065592\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.105655\n",
      "\n",
      "Test set: Average loss: 0.1220, Accuracy: 24015/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.033390\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.110767\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.169465\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.137279\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.107237\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.064661\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.211501\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.157128\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.039070\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.048768\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.214828\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.104374\n",
      "\n",
      "Test set: Average loss: 0.1219, Accuracy: 24001/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.051176\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.099026\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.126509\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.233002\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.113547\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.176996\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.191565\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.076285\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.110580\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.149077\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.236649\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.112807\n",
      "\n",
      "Test set: Average loss: 0.1145, Accuracy: 24078/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.049137\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.100053\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.039854\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.044661\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.008144\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.015057\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.167518\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.053757\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.071401\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.100357\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.142541\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.045072\n",
      "\n",
      "Test set: Average loss: 0.1266, Accuracy: 24006/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.121322\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.049515\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.092568\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.034339\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.045353\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.032497\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.178623\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.058572\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.093628\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.063241\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.084122\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.122804\n",
      "\n",
      "Test set: Average loss: 0.1158, Accuracy: 24070/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.037029\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.189480\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.165282\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.046739\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.110138\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.151344\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.085103\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.120989\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.189304\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.033604\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.173496\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.040164\n",
      "\n",
      "Test set: Average loss: 0.1206, Accuracy: 24060/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.088611\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.046520\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.216936\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.099634\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.015934\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.057584\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.005435\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.052501\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.097567\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.112027\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.212715\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.073339\n",
      "\n",
      "Test set: Average loss: 0.1208, Accuracy: 24054/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.028517\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.193286\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.238364\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.086607\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.137768\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.084753\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.036344\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.188968\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.141525\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.008026\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.098215\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.113857\n",
      "\n",
      "Test set: Average loss: 0.1218, Accuracy: 24049/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.035180\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.139142\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.091495\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.090099\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.065950\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.091226\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.115949\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.103847\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.194930\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.187152\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.062659\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.028506\n",
      "\n",
      "Test set: Average loss: 0.1184, Accuracy: 24068/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.096051\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.007244\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.032858\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.037334\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.078334\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.115184\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.066856\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.161984\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.058098\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.188880\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.153666\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.076368\n",
      "\n",
      "Test set: Average loss: 0.1158, Accuracy: 24078/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.054634\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.074489\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.053178\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.043027\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.013714\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.004629\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.133193\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.059291\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.029221\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.150095\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.034942\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.042383\n",
      "\n",
      "Test set: Average loss: 0.1167, Accuracy: 24084/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.032930\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.087067\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.109160\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.006732\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.195252\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.169051\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.114240\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.086053\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.042605\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.109280\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.054130\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.077096\n",
      "\n",
      "Test set: Average loss: 0.1258, Accuracy: 24045/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.130646\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.037363\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.223925\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.107089\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.088823\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.106582\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.208652\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.113467\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.092546\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.123587\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.136315\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.111026\n",
      "\n",
      "Test set: Average loss: 0.1195, Accuracy: 24099/25000 (96%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.041887\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.031836\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.056715\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.001718\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.043545\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.087793\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.027393\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.067181\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.118332\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.156186\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.241701\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.051411\n",
      "\n",
      "Test set: Average loss: 0.1193, Accuracy: 24107/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.163717\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.117721\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.021191\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.127349\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.068547\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.118532\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.115454\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.060922\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.167756\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.124570\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.035183\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.049703\n",
      "\n",
      "Test set: Average loss: 0.1173, Accuracy: 24091/25000 (96%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.198204\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.070939\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.060564\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.076058\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.051501\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.084563\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.065386\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.028728\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.093639\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.070406\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.052174\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.114177\n",
      "\n",
      "Test set: Average loss: 0.1238, Accuracy: 24077/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.229401\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.078611\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.085049\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.073974\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.133370\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.072565\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.106930\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.038472\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.096167\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.254712\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.071212\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.100410\n",
      "\n",
      "Test set: Average loss: 0.1222, Accuracy: 24114/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.066177\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.068181\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.184361\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.070879\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.010236\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.116809\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.093123\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.008650\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.051515\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.044509\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.072733\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.015334\n",
      "\n",
      "Test set: Average loss: 0.1190, Accuracy: 24100/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.070330\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.022343\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.106445\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.107417\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.016875\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.049169\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.182514\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.022269\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.030896\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.064136\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.020945\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.132850\n",
      "\n",
      "Test set: Average loss: 0.1203, Accuracy: 24115/25000 (96%)\n",
      "\n",
      "Training model with 32 and 64 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.055444\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.109639\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.028182\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.087353\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.194908\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.075768\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.141248\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.185214\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.108589\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.028599\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.085650\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.159488\n",
      "\n",
      "Test set: Average loss: 0.1254, Accuracy: 23981/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.167727\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.095628\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.190725\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.167391\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.136810\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.120032\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.204011\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.030327\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.170472\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.113223\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.138144\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.023498\n",
      "\n",
      "Test set: Average loss: 0.1196, Accuracy: 24032/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.136398\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.114389\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.002303\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.084307\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.056179\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.084320\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.120867\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.109332\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.124788\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.197132\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.244187\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.186021\n",
      "\n",
      "Test set: Average loss: 0.1199, Accuracy: 24021/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.180481\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.038538\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.179331\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.010088\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.046768\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.058916\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.089915\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.074815\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.146368\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.191240\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.080773\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.136762\n",
      "\n",
      "Test set: Average loss: 0.1153, Accuracy: 24067/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.144379\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.055800\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.083646\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.056969\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.158576\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.114419\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.169430\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.085615\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.058355\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.062172\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.228742\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.026870\n",
      "\n",
      "Test set: Average loss: 0.1156, Accuracy: 24072/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.052307\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.093979\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.057500\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.034059\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.086100\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.085225\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.292631\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.014838\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.083472\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.051851\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.083468\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.066718\n",
      "\n",
      "Test set: Average loss: 0.1129, Accuracy: 24071/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.119352\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.061844\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.169175\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.061199\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.062154\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.102254\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.154492\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.076145\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.034565\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.063409\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.099506\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.035823\n",
      "\n",
      "Test set: Average loss: 0.1137, Accuracy: 24110/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.104959\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.058420\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.045395\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.109476\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.126420\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.042887\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.220866\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.103034\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.014794\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.019809\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.060262\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.056921\n",
      "\n",
      "Test set: Average loss: 0.1157, Accuracy: 24084/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.145967\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.079683\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.070303\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.048430\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.093613\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.081501\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.156747\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.103689\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.172728\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.096085\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.202601\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.235096\n",
      "\n",
      "Test set: Average loss: 0.1188, Accuracy: 24096/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.230104\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.064227\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.173007\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.064592\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.132146\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.015007\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.086869\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.069708\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.156024\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.202870\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.054672\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.079236\n",
      "\n",
      "Test set: Average loss: 0.1115, Accuracy: 24126/25000 (97%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.013811\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.077896\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.100921\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.174327\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.032054\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.121528\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.043671\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.044681\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.007845\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.131178\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.022747\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.030530\n",
      "\n",
      "Test set: Average loss: 0.1138, Accuracy: 24126/25000 (97%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.053107\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.065138\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.024714\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.114574\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.065587\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.153474\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.118297\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.131796\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.041887\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.009813\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.099424\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.065782\n",
      "\n",
      "Test set: Average loss: 0.1226, Accuracy: 24098/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.137137\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.040884\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.091097\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.221646\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.077340\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.079645\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.071225\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.072923\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.028081\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.079483\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.065202\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.089928\n",
      "\n",
      "Test set: Average loss: 0.1096, Accuracy: 24112/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.128601\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.095543\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.088592\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.111397\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.049680\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.059982\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.205528\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.093964\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.061092\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.192894\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.094935\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.093360\n",
      "\n",
      "Test set: Average loss: 0.1110, Accuracy: 24140/25000 (97%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.165613\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.024721\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.112545\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.031124\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.088266\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.041283\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.032046\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.101581\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.059533\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.049055\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.059502\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.163241\n",
      "\n",
      "Test set: Average loss: 0.1150, Accuracy: 24151/25000 (97%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.039261\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.106062\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.078801\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.053321\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.036059\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.087734\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.054303\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.100833\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.001561\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.037494\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.124439\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.039707\n",
      "\n",
      "Test set: Average loss: 0.1139, Accuracy: 24146/25000 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.038172\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.049738\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.043064\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.117169\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.025888\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.075392\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.072231\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.005866\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.027363\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.123705\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.222209\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.079713\n",
      "\n",
      "Test set: Average loss: 0.1144, Accuracy: 24145/25000 (97%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.031442\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.014044\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.004040\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.086357\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.050341\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.099789\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.106373\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.023426\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.068063\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.041649\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.010660\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.117256\n",
      "\n",
      "Test set: Average loss: 0.1153, Accuracy: 24136/25000 (97%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.064651\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.031413\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.161518\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.073631\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.036263\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.030436\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.107083\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.103890\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.138013\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.127169\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.079030\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.071467\n",
      "\n",
      "Test set: Average loss: 0.1165, Accuracy: 24125/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.067957\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.149375\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.123242\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.061219\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.035742\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.101902\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.085893\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.001843\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.155921\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.065907\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.077486\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.068970\n",
      "\n",
      "Test set: Average loss: 0.1171, Accuracy: 24124/25000 (96%)\n",
      "\n",
      "Training model with 40 and 80 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.138183\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.060600\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.102947\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.252361\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.038890\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.055704\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.111749\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.143180\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.070452\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.202136\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.159800\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.169696\n",
      "\n",
      "Test set: Average loss: 0.1185, Accuracy: 24043/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.241222\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.124376\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.220007\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.102141\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.092744\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.166058\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.052829\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.113006\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.270413\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.107055\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.121457\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.193066\n",
      "\n",
      "Test set: Average loss: 0.1251, Accuracy: 23991/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.045647\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.083010\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.217792\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.115160\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.145937\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.039122\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.021558\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.096010\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.065691\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.073905\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.202036\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.077751\n",
      "\n",
      "Test set: Average loss: 0.1205, Accuracy: 24041/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.130889\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.266427\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.002655\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.072513\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.162334\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.114888\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.095283\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.030678\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.102413\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.208373\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.081735\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.139367\n",
      "\n",
      "Test set: Average loss: 0.1224, Accuracy: 24032/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.050990\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.241471\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.213016\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.094268\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.263203\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.175840\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.093220\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.200394\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.062160\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.068494\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.078245\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.057950\n",
      "\n",
      "Test set: Average loss: 0.1193, Accuracy: 24022/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.067418\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.130078\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.098044\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.044730\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.095788\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.003988\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.046448\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.058311\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.087204\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.024681\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.036519\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.256290\n",
      "\n",
      "Test set: Average loss: 0.1128, Accuracy: 24077/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.055789\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.097916\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.060794\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.015454\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.080741\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.048199\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.036139\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.017824\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.110975\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.230237\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.238400\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.098822\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 24097/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.110763\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.060204\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.041015\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.070849\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.048933\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.239211\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.061182\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.045057\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.134796\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.030489\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.041398\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.079696\n",
      "\n",
      "Test set: Average loss: 0.1139, Accuracy: 24106/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.081418\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.084797\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.117794\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.064591\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.082055\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.020976\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.110574\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.090439\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.034734\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.136628\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.058543\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.141235\n",
      "\n",
      "Test set: Average loss: 0.1136, Accuracy: 24086/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.089452\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.087174\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.042466\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.121846\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.033980\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.085488\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.091747\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.069874\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.018906\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.035426\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.027842\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.073407\n",
      "\n",
      "Test set: Average loss: 0.1170, Accuracy: 24106/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.070686\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.091633\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.060498\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.117958\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.064780\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.041476\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.025692\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.023007\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.135871\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.020415\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.005718\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.043848\n",
      "\n",
      "Test set: Average loss: 0.1129, Accuracy: 24111/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.052691\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.075041\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.109123\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.037805\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.092449\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.015826\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.016536\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.038649\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.054768\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.113444\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.034478\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.045423\n",
      "\n",
      "Test set: Average loss: 0.1201, Accuracy: 24071/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.111181\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.069130\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.147661\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.081342\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.082953\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.057397\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.001316\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.021111\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.120423\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.111327\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.068761\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.118054\n",
      "\n",
      "Test set: Average loss: 0.1119, Accuracy: 24108/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.035144\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.055815\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.003463\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.022416\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.013480\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.022391\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.086092\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.090841\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.129687\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.147926\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.152922\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.032601\n",
      "\n",
      "Test set: Average loss: 0.1144, Accuracy: 24126/25000 (97%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.097442\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.116848\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.065467\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.041913\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.073437\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.042181\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.092132\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.088941\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.113160\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.028048\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.041840\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.110604\n",
      "\n",
      "Test set: Average loss: 0.1158, Accuracy: 24118/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.057999\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.103609\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.132293\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.179445\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.067377\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.124831\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.061664\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.050511\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.110839\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.019164\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.033539\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.036177\n",
      "\n",
      "Test set: Average loss: 0.1163, Accuracy: 24129/25000 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.087079\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.049114\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.002543\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.048642\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.070435\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.068197\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.045298\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.045082\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.053568\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.034522\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.040574\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.049144\n",
      "\n",
      "Test set: Average loss: 0.1228, Accuracy: 24124/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.053429\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.010113\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.174930\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.019107\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.074860\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.036271\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.011382\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.014712\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.078903\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.025011\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.015207\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.168837\n",
      "\n",
      "Test set: Average loss: 0.1210, Accuracy: 24124/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.029753\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.106798\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.052854\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.068987\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.094274\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.031796\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.237417\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.038081\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.094847\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.038049\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.085710\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.170434\n",
      "\n",
      "Test set: Average loss: 0.1195, Accuracy: 24131/25000 (97%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.011342\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.007310\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.077744\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.071383\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.045143\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.038128\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.026918\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.037374\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.052109\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.010710\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.123596\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.058995\n",
      "\n",
      "Test set: Average loss: 0.1172, Accuracy: 24146/25000 (97%)\n",
      "\n",
      "Training model with 48 and 96 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.251816\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.091908\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.185738\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.016075\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.065113\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.040876\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.047381\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.139328\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.045003\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.211893\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.051824\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.133582\n",
      "\n",
      "Test set: Average loss: 0.1213, Accuracy: 24011/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.210218\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.081539\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.107879\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.105526\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.080826\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.047831\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.154247\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.163440\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.126373\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.125709\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.162367\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.104355\n",
      "\n",
      "Test set: Average loss: 0.1226, Accuracy: 24055/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.156505\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.162340\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.289738\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.080801\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.092338\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.146356\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.090130\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.104322\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.121475\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.011332\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.094839\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.140314\n",
      "\n",
      "Test set: Average loss: 0.1210, Accuracy: 24059/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.256016\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.076474\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.005363\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.127744\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.070434\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.056782\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.100989\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.030107\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.099740\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.097684\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.033430\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.066131\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 24075/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.144741\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.146107\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.074165\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.072013\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.117183\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.160033\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.071097\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.034487\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.095111\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.073315\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.030104\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.090671\n",
      "\n",
      "Test set: Average loss: 0.1145, Accuracy: 24071/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.101294\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.104678\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.109309\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.105112\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.034881\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.046724\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.113014\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.186877\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.029827\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.121189\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.052960\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.149293\n",
      "\n",
      "Test set: Average loss: 0.1161, Accuracy: 24098/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.148065\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.170761\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.090031\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.082150\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.041432\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.105924\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.083725\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.089734\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.098854\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.230525\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.009301\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.077575\n",
      "\n",
      "Test set: Average loss: 0.1134, Accuracy: 24107/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.073490\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[173]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m testAcc = []\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     trainLoss.append(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m.item())\n\u001b[32m     25\u001b[39m     t,a = test(model, device, test_loader, test_criterion)\n\u001b[32m     26\u001b[39m     testLoss.append(t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[169]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, device, train_loader, criterion, optimizer, epoch)\u001b[39m\n\u001b[32m     13\u001b[39m loss = criterion(logits, target)\n\u001b[32m     14\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_idx % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTrain Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m)] Loss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m     19\u001b[39m         epoch, batch_idx * \u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mlen\u001b[39m(train_loader.dataset),\n\u001b[32m     20\u001b[39m         \u001b[32m100.\u001b[39m * batch_idx / \u001b[38;5;28mlen\u001b[39m(train_loader), loss.item()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/optim/lr_scheduler.py:124\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m opt = opt_ref()\n\u001b[32m    123\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/optim/optimizer.py:470\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28mself\u001b[39m = cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    469\u001b[39m profile_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/autograd/profiler.py:771\u001b[39m, in \u001b[36mrecord_function.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m     \u001b[38;5;28mself\u001b[39m.record = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/_ops.py:1158\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=6)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "# Training\n",
    "train_criterion = nn.CrossEntropyLoss() # CrossEntropyLoss combines log-softmax + NLLLoss\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum') # For test function\n",
    "\n",
    "for i in range(nets):\n",
    "    print(f\"Training model with {i*8+8} and {i*16+16} kernels\")\n",
    "    model = cnn_num_kernels_models[i].to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = LambdaLR(optimizer, lambda epoch: 0.95 ** epoch)\n",
    "    trainLoss = []\n",
    "    testLoss = []\n",
    "    testAcc = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        trainLoss.append(train(model, device, train_loader, train_criterion, optimizer, epoch).item())\n",
    "        t,a = test(model, device, test_loader, test_criterion)\n",
    "        testLoss.append(t)\n",
    "        testAcc.append(a)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5cdb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             832\n",
      "         LeakyReLU-2           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
      "            Conv2d-4           [-1, 64, 14, 14]          51,264\n",
      "         LeakyReLU-5           [-1, 64, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 64, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3136]               0\n",
      "            Linear-8                  [-1, 256]         803,072\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 856,453\n",
      "Trainable params: 856,453\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.67\n",
      "Params size (MB): 3.27\n",
      "Estimated Total Size (MB): 3.94\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             832\n",
      "         LeakyReLU-2           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
      "            Conv2d-4           [-1, 64, 14, 14]          51,264\n",
      "         LeakyReLU-5           [-1, 64, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 64, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3136]               0\n",
      "            Linear-8                  [-1, 256]         803,072\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                  [-1, 128]          32,896\n",
      "        LeakyReLU-11                  [-1, 128]               0\n",
      "           Linear-12                    [-1, 5]             645\n",
      "================================================================\n",
      "Total params: 888,709\n",
      "Trainable params: 888,709\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.68\n",
      "Params size (MB): 3.39\n",
      "Estimated Total Size (MB): 4.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nets = 2\n",
    "cnn_num_dense_layer_models = [BaseModule() for _ in range(nets)]\n",
    "\n",
    "for i in range(nets):\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.Conv2d(1, 32, kernel_size=5, padding='same'))\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.Conv2d(32, 64, kernel_size=5, padding='same'))\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.Flatten())\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.Linear(64*7*7, 256))\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "\n",
    "    if i == 0:\n",
    "        cnn_num_dense_layer_models[i].layers.append(nn.Linear(256, 5))\n",
    "    else:\n",
    "        cnn_num_dense_layer_models[i].layers.append(nn.Linear(256, 128))\n",
    "        cnn_num_dense_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "        cnn_num_dense_layer_models[i].layers.append(nn.Linear(128, 5))\n",
    "\n",
    "# Print models\n",
    "for i in range(nets):\n",
    "    summary(cnn_num_dense_layer_models[i], input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 1 dense layers\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 1.613632\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.477903\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.424915\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.404655\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.364250\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.391702\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.446629\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.213461\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.285810\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.225441\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.160558\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.143415\n",
      "\n",
      "Test set: Average loss: 0.2036, Accuracy: 23334/25000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.147010\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.194148\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.317836\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.101981\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.195652\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.324563\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.325317\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.272765\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.269679\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.231284\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.229396\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.355423\n",
      "\n",
      "Test set: Average loss: 0.1829, Accuracy: 23473/25000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.229829\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.133837\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.192589\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.190376\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.271151\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.171697\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.296539\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.210102\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.187278\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.244713\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.244809\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.131538\n",
      "\n",
      "Test set: Average loss: 0.1405, Accuracy: 23803/25000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.190166\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.163804\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.251265\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.162631\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.204078\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.176494\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.088530\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.048984\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.178607\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.244888\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.135804\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.106738\n",
      "\n",
      "Test set: Average loss: 0.1374, Accuracy: 23841/25000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.108460\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.209558\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.236248\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.059397\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.102207\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.103109\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.122030\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.094367\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.193373\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.061355\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.133396\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.118723\n",
      "\n",
      "Test set: Average loss: 0.1435, Accuracy: 23797/25000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.138032\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.094740\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.204522\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.080832\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.262752\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.107502\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.160637\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.085999\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.121954\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.046928\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.153235\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.153671\n",
      "\n",
      "Test set: Average loss: 0.1247, Accuracy: 23916/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.066306\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.067779\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.099560\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.088721\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.211239\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.226241\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.161458\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.198713\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.090521\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.090927\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.116173\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.145616\n",
      "\n",
      "Test set: Average loss: 0.1218, Accuracy: 23975/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.210030\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.086111\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.059003\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.134877\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.313988\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.186975\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.157720\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.092918\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.091718\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.061046\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.085901\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.072417\n",
      "\n",
      "Test set: Average loss: 0.1187, Accuracy: 23988/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.111967\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.073046\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.049293\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.087285\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.188693\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.298778\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.070363\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.042504\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.059644\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.058353\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.042600\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.201761\n",
      "\n",
      "Test set: Average loss: 0.1197, Accuracy: 24018/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.090468\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.082248\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.159181\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.069503\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.121070\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.160461\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.206860\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.195436\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.173128\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.158296\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.131801\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.210057\n",
      "\n",
      "Test set: Average loss: 0.1193, Accuracy: 24031/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.135277\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.023046\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.250759\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.187293\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.074250\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.039527\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.203502\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.061304\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.057913\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.066375\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.111545\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.230240\n",
      "\n",
      "Test set: Average loss: 0.1136, Accuracy: 24086/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.087678\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.023470\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.068280\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.068807\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.090791\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.022991\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.028267\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.064545\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.053927\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.024246\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.063064\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.042055\n",
      "\n",
      "Test set: Average loss: 0.1161, Accuracy: 24086/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.090788\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.131312\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.093634\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.089614\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.084266\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.119887\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.115573\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.022373\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.090323\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.141744\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.177315\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.083096\n",
      "\n",
      "Test set: Average loss: 0.1130, Accuracy: 24093/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.045028\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.041472\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.064454\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.085859\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.105804\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.040852\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.098873\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.072030\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.107726\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.109900\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.259140\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.103857\n",
      "\n",
      "Test set: Average loss: 0.1149, Accuracy: 24050/25000 (96%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.082451\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.064568\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.073705\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.036720\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.112154\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.101298\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.130982\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.084070\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.223236\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.123525\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.131016\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.065567\n",
      "\n",
      "Test set: Average loss: 0.1128, Accuracy: 24087/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.088098\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.082539\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.109252\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.048022\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.154317\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.088578\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.041948\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.203003\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.120200\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.047003\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.084881\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.084847\n",
      "\n",
      "Test set: Average loss: 0.1123, Accuracy: 24080/25000 (96%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.101697\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.181024\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.066278\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.048658\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.119784\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.115126\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.034518\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.061862\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.074563\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.041811\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.043581\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.131163\n",
      "\n",
      "Test set: Average loss: 0.1143, Accuracy: 24077/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.221548\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.067985\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.229062\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.049255\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.012491\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.058752\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.016523\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.018627\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.157796\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.042273\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.019165\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.077483\n",
      "\n",
      "Test set: Average loss: 0.1149, Accuracy: 24102/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.107435\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.092790\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.074272\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.129247\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.184560\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.059472\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.163807\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.060347\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.144715\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.111398\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.030578\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.099722\n",
      "\n",
      "Test set: Average loss: 0.1166, Accuracy: 24085/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.075975\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.083051\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.111011\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.040203\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.147735\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.177297\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.080505\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.064353\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.024907\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.081992\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.111420\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.167253\n",
      "\n",
      "Test set: Average loss: 0.1109, Accuracy: 24095/25000 (96%)\n",
      "\n",
      "Training model with 2 dense layers\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 1.615527\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.490856\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.366632\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.404100\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.387533\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.629421\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.276036\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.345421\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.128223\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.297615\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.350390\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.186934\n",
      "\n",
      "Test set: Average loss: 0.1903, Accuracy: 23393/25000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.196651\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.289714\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.244614\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.210699\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.231048\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.194847\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.369433\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.051694\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.386294\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.135724\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.215555\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.235820\n",
      "\n",
      "Test set: Average loss: 0.1549, Accuracy: 23671/25000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.151126\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.298697\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.172767\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.159749\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.188537\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.118438\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.150547\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.318001\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.262448\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.136963\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.123662\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.105608\n",
      "\n",
      "Test set: Average loss: 0.1402, Accuracy: 23800/25000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.079764\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.103901\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.249326\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.102416\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.109013\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.197036\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.104349\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.153458\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.475075\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.192364\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.185292\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.149607\n",
      "\n",
      "Test set: Average loss: 0.1352, Accuracy: 23867/25000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.157324\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.165787\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.134739\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.076992\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.091149\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.282001\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.285558\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.038059\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.079229\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.133414\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.172702\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.101243\n",
      "\n",
      "Test set: Average loss: 0.1312, Accuracy: 23891/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.194774\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.187167\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.153759\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.352185\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.128602\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.060705\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.149974\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.186332\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.043441\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.168037\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.106592\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.085027\n",
      "\n",
      "Test set: Average loss: 0.1245, Accuracy: 23929/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.184795\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.222902\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.099292\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.137096\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.066962\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.169206\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.196794\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.158907\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.078758\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.059952\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.124700\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.123573\n",
      "\n",
      "Test set: Average loss: 0.1298, Accuracy: 23886/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.120604\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.148136\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.200918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m testAcc = []\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     trainLoss.append(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m.item())\n\u001b[32m     25\u001b[39m     t,a = test(model, device, test_loader, test_criterion)\n\u001b[32m     26\u001b[39m     testLoss.append(t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, device, train_loader, criterion, optimizer, epoch)\u001b[39m\n\u001b[32m     12\u001b[39m logits = model(data)\n\u001b[32m     13\u001b[39m loss = criterion(logits, target)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m optimizer.step()\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_idx % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=6)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "# Training\n",
    "train_criterion = nn.CrossEntropyLoss() # CrossEntropyLoss combines log-softmax + NLLLoss\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum') # For test function\n",
    "\n",
    "for i in range(nets):\n",
    "    print(f\"Training model with {i+1} dense layers\")\n",
    "    model = cnn_num_dense_layer_models[i].to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = LambdaLR(optimizer, lambda epoch: 0.95 ** epoch)\n",
    "    trainLoss = []\n",
    "    testLoss = []\n",
    "    testAcc = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        trainLoss.append(train(model, device, train_loader, train_criterion, optimizer, epoch).item())\n",
    "        t,a = test(model, device, test_loader, test_criterion)\n",
    "        testLoss.append(t)\n",
    "        testAcc.append(a)\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
