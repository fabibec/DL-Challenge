{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3797d8b",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "First of all we need to properly prepare our data. Therefor I implemented a PyTorch Dataset for the data to be conveniently accessed during training and validation. \n",
    "\n",
    "Moreover I performed some data augmentation, by applying a random horizontal flip and a random rotation. \n",
    "I choose to not use a horizontal flip, because for example in the case of baskets this would destroy the meaning of the image. \n",
    "Since the images are very low resolution gray scale images i decided to not apply any additional noise and color transforms.\n",
    "\n",
    "Stats of the data:\n",
    "    - 5 classes, 10.000 images in training set, 5.000 in test set\n",
    "    - 28 x 28 images -> 784 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d505b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.RandomHorizontalFlip(p = 0.5),\n",
    "    v2.RandomRotation(degrees = 10),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Width and height shift\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "test_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf9c2991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x900 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import polars as pl\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Classification targets & subfolder names\n",
    "classes = {\n",
    "    0: 'basket',\n",
    "    1: 'eye',\n",
    "    2: 'binoculars',\n",
    "    3: 'rabbit',\n",
    "    4: 'hand',\n",
    "}\n",
    "\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = pl.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.img_labels.row(idx)[1:]\n",
    "        img_path = os.path.join(self.img_dir, img_path)\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Create Train and Test Datasets\n",
    "training_data = QuickDrawDataset('dataset/train.csv', 'dataset/images', transforms)\n",
    "test_data = QuickDrawDataset('dataset/test.csv', 'dataset/images', test_transforms)\n",
    "\n",
    "# Sample 5 random images to check if the datasets work\n",
    "plt.figure(figsize=(18,9))\n",
    "for i in range(0,5):\n",
    "    ax= plt.subplot(1,5 ,i+1)\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    fig = ax.imshow(img.squeeze(0), cmap='gray')  # Remove the channel dimension\n",
    "    plt.title(classes[label])\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8006fd",
   "metadata": {},
   "source": [
    "### General Preparation\n",
    "\n",
    "- Import libraries\n",
    "- Set the device\n",
    "- build a \"base\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef7abbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class BaseModule(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18fd30",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "Since the dataset is very similar to the MNIST dataset I first want to explore training a CNN from scratch to get a baseline for the performance.\n",
    "\n",
    "Moreover I found this kaggle notebook that evaluated some choices regarding the CNN architecture. In the following I will try to replicate these experiments on this dataset.\n",
    "\n",
    "Reference:\n",
    " - [Kaggle Notebook](https://www.kaggle.com/code/cdeotte/how-to-choose-cnn-architecture-mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2c6c8",
   "metadata": {},
   "source": [
    "#### Test 1: Amount of Convolution + Pooling Layers\n",
    "\n",
    "Based on the referenced notebook I want to check how many of these layer combinations produce the best output. More layers would result in very small images that will likely not contain any valuable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4414b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 28, 28]             624\n",
      "         LeakyReLU-2           [-1, 24, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 24, 14, 14]               0\n",
      "           Flatten-4                 [-1, 4704]               0\n",
      "            Linear-5                  [-1, 256]       1,204,480\n",
      "         LeakyReLU-6                  [-1, 256]               0\n",
      "            Linear-7                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 1,206,389\n",
      "Trainable params: 1,206,389\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 4.60\n",
      "Estimated Total Size (MB): 4.97\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 28, 28]             624\n",
      "         LeakyReLU-2           [-1, 24, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 24, 14, 14]               0\n",
      "            Conv2d-4           [-1, 48, 14, 14]          28,848\n",
      "         LeakyReLU-5           [-1, 48, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 48, 7, 7]               0\n",
      "           Flatten-7                 [-1, 2352]               0\n",
      "            Linear-8                  [-1, 256]         602,368\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 633,125\n",
      "Trainable params: 633,125\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.51\n",
      "Params size (MB): 2.42\n",
      "Estimated Total Size (MB): 2.92\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 28, 28]             624\n",
      "         LeakyReLU-2           [-1, 24, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 24, 14, 14]               0\n",
      "            Conv2d-4           [-1, 48, 14, 14]          28,848\n",
      "         LeakyReLU-5           [-1, 48, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 48, 7, 7]               0\n",
      "            Conv2d-7             [-1, 64, 7, 7]          76,864\n",
      "         LeakyReLU-8             [-1, 64, 7, 7]               0\n",
      "         MaxPool2d-9             [-1, 64, 3, 3]               0\n",
      "          Flatten-10                  [-1, 576]               0\n",
      "           Linear-11                  [-1, 256]         147,712\n",
      "        LeakyReLU-12                  [-1, 256]               0\n",
      "           Linear-13                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 255,333\n",
      "Trainable params: 255,333\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.55\n",
      "Params size (MB): 0.97\n",
      "Estimated Total Size (MB): 1.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build the three CNN models\n",
    "nets = 3\n",
    "cnn_conv_layer_models = [BaseModule() for _ in range(nets)]\n",
    "for i in range(nets):\n",
    "\n",
    "    # First Convolutional Layer (in every model)\n",
    "    cnn_conv_layer_models[i].layers.append(nn.Conv2d(1, 24, kernel_size=5, padding='same'))\n",
    "    cnn_conv_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_conv_layer_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    if i > 0:\n",
    "        # Second Convolutional Layer (in every model except the first)\n",
    "        cnn_conv_layer_models[i].layers.append(nn.Conv2d(24, 48, kernel_size=5, padding='same'))\n",
    "        cnn_conv_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "        cnn_conv_layer_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Third Convolutional Layer (only in the third model)\n",
    "        if i == 2:\n",
    "            cnn_conv_layer_models[i].layers.append(nn.Conv2d(48, 64, kernel_size=5, padding='same'))\n",
    "            cnn_conv_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "            cnn_conv_layer_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    # Output sizes after convolutional layers\n",
    "    # 28x28 -> 14x14 -> 7x7 -> 3x3\n",
    "    # 24*14*14 -> 48*7*7 -> 64*3*3\n",
    "    conv_out = [24*14*14, 48*7*7, 64*3*3]\n",
    "\n",
    "    cnn_conv_layer_models[i].layers.append(nn.Flatten())\n",
    "    cnn_conv_layer_models[i].layers.append(nn.Linear(conv_out[i], 256))\n",
    "    cnn_conv_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_conv_layer_models[i].layers.append(nn.Linear(256, 5))\n",
    "\n",
    "# Print models\n",
    "for i in range(nets):\n",
    "    summary(cnn_conv_layer_models[i], input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce047133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Train and test functions\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data)\n",
    "            test_loss += criterion(logits, target).item()  # sum up batch loss\n",
    "            preds = F.log_softmax(logits, dim=1)\n",
    "            pred = preds.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),test_acc\n",
    "        ))\n",
    "    return test_loss,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e26d0bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 1 conv + pool layers\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m scheduler = LambdaLR(optimizer, \u001b[38;5;28;01mlambda\u001b[39;00m epoch: \u001b[32m0.95\u001b[39m ** epoch)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m.item()\n\u001b[32m     28\u001b[39m     _, a = test(model, device, test_loader, test_criterion)\n\u001b[32m     29\u001b[39m     testAcc.append(a)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, device, train_loader, criterion, optimizer, epoch)\u001b[39m\n\u001b[32m     10\u001b[39m data, target = data.to(device), target.to(device)\n\u001b[32m     11\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m loss = criterion(logits, target)\n\u001b[32m     14\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mBaseModule.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=6)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "# Training\n",
    "train_criterion = nn.CrossEntropyLoss() # CrossEntropyLoss combines log-softmax + NLLLoss\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum') # For test function\n",
    "\n",
    "testAcc = []\n",
    "\n",
    "for i in range(nets):\n",
    "    print(f\"Training model with {i+1} conv + pool layers\")\n",
    "    model = cnn_conv_layer_models[i].to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = LambdaLR(optimizer, lambda epoch: 0.95 ** epoch)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(model, device, train_loader, train_criterion, optimizer, epoch).item()\n",
    "        _, a = test(model, device, test_loader, test_criterion)\n",
    "        testAcc.append(a)\n",
    "        scheduler.step()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for i in range(nets):\n",
    "    plt.plot(history[i].history['val_acc'],linestyle=styles[i])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(names, loc='upper left')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.89,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503afdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 28, 28]             208\n",
      "         LeakyReLU-2            [-1, 8, 28, 28]               0\n",
      "         MaxPool2d-3            [-1, 8, 14, 14]               0\n",
      "            Conv2d-4           [-1, 16, 14, 14]           3,216\n",
      "         LeakyReLU-5           [-1, 16, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 16, 7, 7]               0\n",
      "           Flatten-7                  [-1, 784]               0\n",
      "            Linear-8                  [-1, 256]         200,960\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 205,669\n",
      "Trainable params: 205,669\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.17\n",
      "Params size (MB): 0.78\n",
      "Estimated Total Size (MB): 0.96\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "         LeakyReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]          12,832\n",
      "         LeakyReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "           Flatten-7                 [-1, 1568]               0\n",
      "            Linear-8                  [-1, 256]         401,664\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 416,197\n",
      "Trainable params: 416,197\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.34\n",
      "Params size (MB): 1.59\n",
      "Estimated Total Size (MB): 1.93\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 28, 28]             624\n",
      "         LeakyReLU-2           [-1, 24, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 24, 14, 14]               0\n",
      "            Conv2d-4           [-1, 48, 14, 14]          28,848\n",
      "         LeakyReLU-5           [-1, 48, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 48, 7, 7]               0\n",
      "           Flatten-7                 [-1, 2352]               0\n",
      "            Linear-8                  [-1, 256]         602,368\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 633,125\n",
      "Trainable params: 633,125\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.51\n",
      "Params size (MB): 2.42\n",
      "Estimated Total Size (MB): 2.92\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             832\n",
      "         LeakyReLU-2           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
      "            Conv2d-4           [-1, 64, 14, 14]          51,264\n",
      "         LeakyReLU-5           [-1, 64, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 64, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3136]               0\n",
      "            Linear-8                  [-1, 256]         803,072\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 856,453\n",
      "Trainable params: 856,453\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.67\n",
      "Params size (MB): 3.27\n",
      "Estimated Total Size (MB): 3.94\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 40, 28, 28]           1,040\n",
      "         LeakyReLU-2           [-1, 40, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 40, 14, 14]               0\n",
      "            Conv2d-4           [-1, 80, 14, 14]          80,080\n",
      "         LeakyReLU-5           [-1, 80, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 80, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3920]               0\n",
      "            Linear-8                  [-1, 256]       1,003,776\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 1,086,181\n",
      "Trainable params: 1,086,181\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.84\n",
      "Params size (MB): 4.14\n",
      "Estimated Total Size (MB): 4.99\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 48, 28, 28]           1,248\n",
      "         LeakyReLU-2           [-1, 48, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 48, 14, 14]               0\n",
      "            Conv2d-4           [-1, 96, 14, 14]         115,296\n",
      "         LeakyReLU-5           [-1, 96, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 96, 7, 7]               0\n",
      "           Flatten-7                 [-1, 4704]               0\n",
      "            Linear-8                  [-1, 256]       1,204,480\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 1,322,309\n",
      "Trainable params: 1,322,309\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.01\n",
      "Params size (MB): 5.04\n",
      "Estimated Total Size (MB): 6.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nets = 6\n",
    "cnn_num_kernels_models = [BaseModule() for _ in range(nets)]\n",
    "for i in range(nets):\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Conv2d(1, i*8+8, kernel_size=5, padding='same'))\n",
    "    cnn_num_kernels_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_kernels_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Conv2d(i*8+8, i*16+16, kernel_size=5, padding='same'))\n",
    "    cnn_num_kernels_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_kernels_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Flatten())\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Linear((i*16+16)*7*7, 256))\n",
    "    cnn_num_kernels_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_kernels_models[i].layers.append(nn.Linear(256, 5))\n",
    "\n",
    "# Print models\n",
    "for i in range(nets):\n",
    "    summary(cnn_num_kernels_models[i], input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41236cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 8 and 16 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.333955\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.344451\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.098977\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.232188\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.220662\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.130773\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.267109\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.113791\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.269854\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.093394\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.152315\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.209864\n",
      "\n",
      "Test set: Average loss: 0.1552, Accuracy: 23685/25000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.308205\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.399003\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.205928\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.154390\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.236444\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.232380\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.193305\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.174637\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.165314\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.098730\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.211798\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.177017\n",
      "\n",
      "Test set: Average loss: 0.1500, Accuracy: 23746/25000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.087508\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.217780\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.144492\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.108917\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.120703\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.089090\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.082845\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.266603\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.149023\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.164099\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.084195\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.219911\n",
      "\n",
      "Test set: Average loss: 0.1468, Accuracy: 23830/25000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.207574\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.163471\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.181908\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.263677\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.102562\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.135236\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.037650\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.089728\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.246489\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.059160\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.134700\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.180462\n",
      "\n",
      "Test set: Average loss: 0.1417, Accuracy: 23829/25000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.165644\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.132120\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.183984\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.054330\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.167584\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.191885\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.119254\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.058699\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.209246\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.091368\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.197240\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.294336\n",
      "\n",
      "Test set: Average loss: 0.1441, Accuracy: 23806/25000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.151534\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.251693\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.318758\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.279291\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.164991\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.078796\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.135709\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.261902\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.058261\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.072398\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.225287\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.089774\n",
      "\n",
      "Test set: Average loss: 0.1429, Accuracy: 23828/25000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.170296\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.124595\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.160630\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.167571\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.092782\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.151620\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.148043\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.127491\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.135988\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.181135\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.066734\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.159083\n",
      "\n",
      "Test set: Average loss: 0.1361, Accuracy: 23859/25000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.078976\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.114087\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.177859\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.078379\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.227821\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.143388\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.249558\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.094018\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.321006\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.200644\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.200470\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.250143\n",
      "\n",
      "Test set: Average loss: 0.1433, Accuracy: 23851/25000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.135130\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.121038\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.031195\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.092741\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.192206\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.096763\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.093716\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.072064\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.022808\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.183738\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.144056\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.061048\n",
      "\n",
      "Test set: Average loss: 0.1368, Accuracy: 23881/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.085463\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.106245\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.252396\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.127964\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.039561\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.062052\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.066052\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.036236\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.309119\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.218100\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.081046\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.070081\n",
      "\n",
      "Test set: Average loss: 0.1349, Accuracy: 23917/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.163916\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.323503\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.089310\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.070442\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.092544\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.283420\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.178493\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.155971\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.144651\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.151739\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.262201\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.171850\n",
      "\n",
      "Test set: Average loss: 0.1370, Accuracy: 23888/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.212582\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.287183\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.091193\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.150844\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.042034\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.046944\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.143650\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.146631\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.086582\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.095720\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.156922\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.063231\n",
      "\n",
      "Test set: Average loss: 0.1312, Accuracy: 23946/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.184643\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.084735\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.048874\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.254733\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.177864\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.048778\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.120090\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.097496\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.134071\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.084002\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.137642\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.144489\n",
      "\n",
      "Test set: Average loss: 0.1307, Accuracy: 23938/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.123217\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.201438\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.069475\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.164084\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.080212\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.125573\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.143792\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.259593\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.260826\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.072840\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.113005\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.048880\n",
      "\n",
      "Test set: Average loss: 0.1323, Accuracy: 23910/25000 (96%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.149295\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.058519\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.197346\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.232054\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.191716\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.023679\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.057996\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.283589\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.161991\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.116619\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.153608\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.139731\n",
      "\n",
      "Test set: Average loss: 0.1296, Accuracy: 23947/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.144081\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.133690\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.139744\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.036710\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.133875\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.232994\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.177454\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.318568\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.090175\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.094679\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.027220\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.063837\n",
      "\n",
      "Test set: Average loss: 0.1321, Accuracy: 23943/25000 (96%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.150816\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.122451\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.028721\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.127981\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.241953\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.045705\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.067133\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.179816\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.229336\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.110989\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.085171\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.079105\n",
      "\n",
      "Test set: Average loss: 0.1281, Accuracy: 23933/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.078493\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.066006\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.199797\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.031890\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.118610\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.099817\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.112308\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.235012\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.168877\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.168540\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.135744\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.024670\n",
      "\n",
      "Test set: Average loss: 0.1279, Accuracy: 23980/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.108661\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.110904\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.043129\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.073658\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.080033\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.228351\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.070859\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.149975\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.163319\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.041222\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.078755\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.108200\n",
      "\n",
      "Test set: Average loss: 0.1281, Accuracy: 23975/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.144213\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.094141\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.201630\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.175212\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.109980\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.095178\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.223696\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.060148\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.266226\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.034557\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.068445\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.068549\n",
      "\n",
      "Test set: Average loss: 0.1301, Accuracy: 23943/25000 (96%)\n",
      "\n",
      "Training model with 16 and 32 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.254799\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.080431\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.029685\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.209175\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.069811\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.068772\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.201683\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.088097\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.093030\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.091311\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.087720\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.161325\n",
      "\n",
      "Test set: Average loss: 0.1299, Accuracy: 23919/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.167689\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.213643\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.165459\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.149016\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.162819\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.228655\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.228755\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.131897\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.119662\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.068808\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.264149\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.065477\n",
      "\n",
      "Test set: Average loss: 0.1355, Accuracy: 23925/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.039603\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.180502\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.171913\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.171453\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.359618\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.260225\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.118950\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.066617\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.118902\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.117335\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.232123\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.124574\n",
      "\n",
      "Test set: Average loss: 0.1323, Accuracy: 23951/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.139779\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.075239\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.144170\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.105706\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.049003\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.076510\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.204166\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.120016\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.101648\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.158649\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.111316\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.154502\n",
      "\n",
      "Test set: Average loss: 0.1303, Accuracy: 23964/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.116690\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.114921\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.107097\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.054581\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.286050\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.091979\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.063270\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.137498\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.495823\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.115970\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.054712\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.054834\n",
      "\n",
      "Test set: Average loss: 0.1272, Accuracy: 23924/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.137932\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.120279\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.038741\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.213415\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.106443\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.299545\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.154277\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.079220\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.140779\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.122885\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.140998\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.120080\n",
      "\n",
      "Test set: Average loss: 0.1218, Accuracy: 24006/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.130154\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.127197\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.100948\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.076655\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.093308\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.151603\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.086434\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.061263\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.132264\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.052390\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.057039\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.170614\n",
      "\n",
      "Test set: Average loss: 0.1309, Accuracy: 23958/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.084877\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.085686\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.111528\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.104802\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.024131\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.110804\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.124951\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.157204\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.094040\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.055111\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.205246\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.173539\n",
      "\n",
      "Test set: Average loss: 0.1251, Accuracy: 23993/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.286447\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.028401\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.158259\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.022113\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.096909\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.014838\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.059709\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.143194\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.100293\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.112451\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.097647\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.057879\n",
      "\n",
      "Test set: Average loss: 0.1225, Accuracy: 24020/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.128305\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.121586\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.192818\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.219819\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.161008\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.052272\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.036903\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.065321\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.118224\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.135699\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.096491\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.084325\n",
      "\n",
      "Test set: Average loss: 0.1220, Accuracy: 24023/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.058296\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.085352\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.042994\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.159410\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.037746\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.144676\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.096840\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.050933\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.041832\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.087011\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.100266\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.130467\n",
      "\n",
      "Test set: Average loss: 0.1251, Accuracy: 23999/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.097775\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.079980\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.027274\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.058802\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.070667\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.088731\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.154716\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.026565\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.031665\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.245646\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.074630\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.126159\n",
      "\n",
      "Test set: Average loss: 0.1170, Accuracy: 24070/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.123365\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.107042\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.034474\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.062620\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.096784\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.179606\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.067048\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.124941\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.063974\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.056302\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.107978\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.048954\n",
      "\n",
      "Test set: Average loss: 0.1176, Accuracy: 24034/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.081366\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.077099\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.073028\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.018187\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.207785\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.125393\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.271130\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.117772\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.095885\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.130177\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.119643\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.057736\n",
      "\n",
      "Test set: Average loss: 0.1186, Accuracy: 24055/25000 (96%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.032929\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.182938\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.130040\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.165130\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.060429\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.082875\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.067303\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.022743\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.078607\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.021577\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.042529\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.046930\n",
      "\n",
      "Test set: Average loss: 0.1194, Accuracy: 24050/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.112571\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.200747\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.027154\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.042198\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.064204\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.059792\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.066278\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.118229\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.154344\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.030210\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.175307\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.019164\n",
      "\n",
      "Test set: Average loss: 0.1225, Accuracy: 24042/25000 (96%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.073124\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.138784\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.065448\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.081478\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.284068\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.158661\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.018851\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.074684\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.186644\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.084865\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.079102\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.164263\n",
      "\n",
      "Test set: Average loss: 0.1186, Accuracy: 24051/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.090278\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.077388\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.111468\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.018161\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.028266\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.146465\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.101655\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.118849\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.171212\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.176815\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.090488\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.084201\n",
      "\n",
      "Test set: Average loss: 0.1194, Accuracy: 24058/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.047630\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.160799\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.105260\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.050428\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.080825\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.108745\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.058420\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.089105\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.109304\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.168278\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.050449\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.108318\n",
      "\n",
      "Test set: Average loss: 0.1168, Accuracy: 24087/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.159643\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.100357\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.145246\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.046407\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.089418\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.060134\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.165257\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.133830\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.010786\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.030480\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.095689\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.073547\n",
      "\n",
      "Test set: Average loss: 0.1226, Accuracy: 24045/25000 (96%)\n",
      "\n",
      "Training model with 24 and 48 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.142370\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.152594\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.097316\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.259115\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.107201\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.063849\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.129817\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.162869\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.095971\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.153145\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.155123\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.117100\n",
      "\n",
      "Test set: Average loss: 0.1268, Accuracy: 23966/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.072739\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.215588\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.068119\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.073014\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.180380\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.121357\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.241472\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.066531\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.277672\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.074038\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.065592\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.105655\n",
      "\n",
      "Test set: Average loss: 0.1220, Accuracy: 24015/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.033390\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.110767\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.169465\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.137279\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.107237\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.064661\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.211501\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.157128\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.039070\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.048768\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.214828\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.104374\n",
      "\n",
      "Test set: Average loss: 0.1219, Accuracy: 24001/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.051176\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.099026\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.126509\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.233002\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.113547\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.176996\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.191565\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.076285\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.110580\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.149077\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.236649\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.112807\n",
      "\n",
      "Test set: Average loss: 0.1145, Accuracy: 24078/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.049137\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.100053\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.039854\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.044661\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.008144\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.015057\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.167518\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.053757\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.071401\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.100357\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.142541\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.045072\n",
      "\n",
      "Test set: Average loss: 0.1266, Accuracy: 24006/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.121322\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.049515\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.092568\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.034339\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.045353\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.032497\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.178623\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.058572\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.093628\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.063241\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.084122\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.122804\n",
      "\n",
      "Test set: Average loss: 0.1158, Accuracy: 24070/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.037029\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.189480\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.165282\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.046739\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.110138\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.151344\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.085103\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.120989\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.189304\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.033604\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.173496\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.040164\n",
      "\n",
      "Test set: Average loss: 0.1206, Accuracy: 24060/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.088611\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.046520\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.216936\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.099634\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.015934\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.057584\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.005435\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.052501\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.097567\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.112027\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.212715\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.073339\n",
      "\n",
      "Test set: Average loss: 0.1208, Accuracy: 24054/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.028517\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.193286\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.238364\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.086607\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.137768\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.084753\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.036344\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.188968\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.141525\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.008026\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.098215\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.113857\n",
      "\n",
      "Test set: Average loss: 0.1218, Accuracy: 24049/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.035180\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.139142\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.091495\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.090099\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.065950\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.091226\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.115949\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.103847\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.194930\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.187152\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.062659\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.028506\n",
      "\n",
      "Test set: Average loss: 0.1184, Accuracy: 24068/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.096051\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.007244\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.032858\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.037334\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.078334\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.115184\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.066856\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.161984\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.058098\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.188880\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.153666\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.076368\n",
      "\n",
      "Test set: Average loss: 0.1158, Accuracy: 24078/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.054634\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.074489\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.053178\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.043027\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.013714\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.004629\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.133193\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.059291\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.029221\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.150095\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.034942\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.042383\n",
      "\n",
      "Test set: Average loss: 0.1167, Accuracy: 24084/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.032930\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.087067\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.109160\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.006732\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.195252\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.169051\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.114240\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.086053\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.042605\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.109280\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.054130\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.077096\n",
      "\n",
      "Test set: Average loss: 0.1258, Accuracy: 24045/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.130646\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.037363\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.223925\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.107089\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.088823\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.106582\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.208652\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.113467\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.092546\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.123587\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.136315\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.111026\n",
      "\n",
      "Test set: Average loss: 0.1195, Accuracy: 24099/25000 (96%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.041887\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.031836\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.056715\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.001718\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.043545\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.087793\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.027393\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.067181\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.118332\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.156186\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.241701\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.051411\n",
      "\n",
      "Test set: Average loss: 0.1193, Accuracy: 24107/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.163717\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.117721\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.021191\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.127349\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.068547\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.118532\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.115454\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.060922\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.167756\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.124570\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.035183\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.049703\n",
      "\n",
      "Test set: Average loss: 0.1173, Accuracy: 24091/25000 (96%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.198204\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.070939\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.060564\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.076058\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.051501\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.084563\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.065386\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.028728\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.093639\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.070406\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.052174\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.114177\n",
      "\n",
      "Test set: Average loss: 0.1238, Accuracy: 24077/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.229401\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.078611\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.085049\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.073974\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.133370\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.072565\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.106930\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.038472\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.096167\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.254712\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.071212\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.100410\n",
      "\n",
      "Test set: Average loss: 0.1222, Accuracy: 24114/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.066177\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.068181\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.184361\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.070879\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.010236\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.116809\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.093123\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.008650\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.051515\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.044509\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.072733\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.015334\n",
      "\n",
      "Test set: Average loss: 0.1190, Accuracy: 24100/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.070330\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.022343\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.106445\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.107417\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.016875\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.049169\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.182514\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.022269\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.030896\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.064136\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.020945\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.132850\n",
      "\n",
      "Test set: Average loss: 0.1203, Accuracy: 24115/25000 (96%)\n",
      "\n",
      "Training model with 32 and 64 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.055444\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.109639\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.028182\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.087353\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.194908\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.075768\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.141248\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.185214\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.108589\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.028599\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.085650\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.159488\n",
      "\n",
      "Test set: Average loss: 0.1254, Accuracy: 23981/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.167727\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.095628\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.190725\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.167391\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.136810\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.120032\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.204011\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.030327\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.170472\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.113223\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.138144\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.023498\n",
      "\n",
      "Test set: Average loss: 0.1196, Accuracy: 24032/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.136398\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.114389\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.002303\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.084307\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.056179\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.084320\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.120867\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.109332\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.124788\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.197132\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.244187\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.186021\n",
      "\n",
      "Test set: Average loss: 0.1199, Accuracy: 24021/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.180481\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.038538\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.179331\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.010088\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.046768\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.058916\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.089915\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.074815\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.146368\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.191240\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.080773\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.136762\n",
      "\n",
      "Test set: Average loss: 0.1153, Accuracy: 24067/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.144379\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.055800\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.083646\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.056969\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.158576\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.114419\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.169430\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.085615\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.058355\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.062172\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.228742\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.026870\n",
      "\n",
      "Test set: Average loss: 0.1156, Accuracy: 24072/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.052307\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.093979\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.057500\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.034059\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.086100\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.085225\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.292631\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.014838\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.083472\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.051851\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.083468\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.066718\n",
      "\n",
      "Test set: Average loss: 0.1129, Accuracy: 24071/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.119352\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.061844\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.169175\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.061199\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.062154\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.102254\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.154492\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.076145\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.034565\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.063409\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.099506\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.035823\n",
      "\n",
      "Test set: Average loss: 0.1137, Accuracy: 24110/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.104959\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.058420\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.045395\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.109476\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.126420\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.042887\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.220866\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.103034\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.014794\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.019809\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.060262\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.056921\n",
      "\n",
      "Test set: Average loss: 0.1157, Accuracy: 24084/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.145967\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.079683\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.070303\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.048430\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.093613\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.081501\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.156747\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.103689\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.172728\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.096085\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.202601\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.235096\n",
      "\n",
      "Test set: Average loss: 0.1188, Accuracy: 24096/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.230104\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.064227\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.173007\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.064592\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.132146\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.015007\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.086869\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.069708\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.156024\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.202870\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.054672\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.079236\n",
      "\n",
      "Test set: Average loss: 0.1115, Accuracy: 24126/25000 (97%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.013811\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.077896\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.100921\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.174327\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.032054\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.121528\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.043671\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.044681\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.007845\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.131178\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.022747\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.030530\n",
      "\n",
      "Test set: Average loss: 0.1138, Accuracy: 24126/25000 (97%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.053107\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.065138\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.024714\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.114574\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.065587\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.153474\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.118297\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.131796\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.041887\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.009813\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.099424\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.065782\n",
      "\n",
      "Test set: Average loss: 0.1226, Accuracy: 24098/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.137137\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.040884\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.091097\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.221646\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.077340\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.079645\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.071225\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.072923\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.028081\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.079483\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.065202\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.089928\n",
      "\n",
      "Test set: Average loss: 0.1096, Accuracy: 24112/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.128601\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.095543\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.088592\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.111397\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.049680\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.059982\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.205528\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.093964\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.061092\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.192894\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.094935\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.093360\n",
      "\n",
      "Test set: Average loss: 0.1110, Accuracy: 24140/25000 (97%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.165613\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.024721\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.112545\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.031124\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.088266\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.041283\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.032046\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.101581\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.059533\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.049055\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.059502\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.163241\n",
      "\n",
      "Test set: Average loss: 0.1150, Accuracy: 24151/25000 (97%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.039261\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.106062\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.078801\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.053321\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.036059\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.087734\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.054303\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.100833\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.001561\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.037494\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.124439\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.039707\n",
      "\n",
      "Test set: Average loss: 0.1139, Accuracy: 24146/25000 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.038172\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.049738\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.043064\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.117169\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.025888\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.075392\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.072231\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.005866\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.027363\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.123705\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.222209\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.079713\n",
      "\n",
      "Test set: Average loss: 0.1144, Accuracy: 24145/25000 (97%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.031442\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.014044\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.004040\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.086357\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.050341\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.099789\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.106373\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.023426\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.068063\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.041649\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.010660\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.117256\n",
      "\n",
      "Test set: Average loss: 0.1153, Accuracy: 24136/25000 (97%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.064651\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.031413\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.161518\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.073631\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.036263\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.030436\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.107083\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.103890\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.138013\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.127169\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.079030\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.071467\n",
      "\n",
      "Test set: Average loss: 0.1165, Accuracy: 24125/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.067957\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.149375\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.123242\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.061219\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.035742\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.101902\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.085893\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.001843\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.155921\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.065907\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.077486\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.068970\n",
      "\n",
      "Test set: Average loss: 0.1171, Accuracy: 24124/25000 (96%)\n",
      "\n",
      "Training model with 40 and 80 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.138183\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.060600\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.102947\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.252361\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.038890\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.055704\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.111749\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.143180\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.070452\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.202136\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.159800\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.169696\n",
      "\n",
      "Test set: Average loss: 0.1185, Accuracy: 24043/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.241222\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.124376\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.220007\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.102141\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.092744\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.166058\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.052829\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.113006\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.270413\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.107055\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.121457\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.193066\n",
      "\n",
      "Test set: Average loss: 0.1251, Accuracy: 23991/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.045647\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.083010\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.217792\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.115160\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.145937\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.039122\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.021558\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.096010\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.065691\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.073905\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.202036\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.077751\n",
      "\n",
      "Test set: Average loss: 0.1205, Accuracy: 24041/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.130889\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.266427\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.002655\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.072513\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.162334\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.114888\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.095283\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.030678\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.102413\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.208373\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.081735\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.139367\n",
      "\n",
      "Test set: Average loss: 0.1224, Accuracy: 24032/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.050990\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.241471\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.213016\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.094268\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.263203\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.175840\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.093220\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.200394\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.062160\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.068494\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.078245\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.057950\n",
      "\n",
      "Test set: Average loss: 0.1193, Accuracy: 24022/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.067418\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.130078\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.098044\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.044730\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.095788\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.003988\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.046448\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.058311\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.087204\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.024681\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.036519\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.256290\n",
      "\n",
      "Test set: Average loss: 0.1128, Accuracy: 24077/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.055789\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.097916\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.060794\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.015454\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.080741\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.048199\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.036139\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.017824\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.110975\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.230237\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.238400\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.098822\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 24097/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.110763\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.060204\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.041015\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.070849\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.048933\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.239211\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.061182\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.045057\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.134796\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.030489\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.041398\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.079696\n",
      "\n",
      "Test set: Average loss: 0.1139, Accuracy: 24106/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.081418\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.084797\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.117794\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.064591\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.082055\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.020976\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.110574\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.090439\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.034734\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.136628\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.058543\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.141235\n",
      "\n",
      "Test set: Average loss: 0.1136, Accuracy: 24086/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.089452\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.087174\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.042466\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.121846\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.033980\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.085488\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.091747\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.069874\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.018906\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.035426\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.027842\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.073407\n",
      "\n",
      "Test set: Average loss: 0.1170, Accuracy: 24106/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.070686\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.091633\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.060498\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.117958\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.064780\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.041476\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.025692\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.023007\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.135871\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.020415\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.005718\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.043848\n",
      "\n",
      "Test set: Average loss: 0.1129, Accuracy: 24111/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.052691\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.075041\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.109123\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.037805\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.092449\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.015826\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.016536\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.038649\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.054768\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.113444\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.034478\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.045423\n",
      "\n",
      "Test set: Average loss: 0.1201, Accuracy: 24071/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.111181\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.069130\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.147661\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.081342\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.082953\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.057397\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.001316\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.021111\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.120423\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.111327\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.068761\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.118054\n",
      "\n",
      "Test set: Average loss: 0.1119, Accuracy: 24108/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.035144\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.055815\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.003463\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.022416\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.013480\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.022391\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.086092\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.090841\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.129687\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.147926\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.152922\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.032601\n",
      "\n",
      "Test set: Average loss: 0.1144, Accuracy: 24126/25000 (97%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.097442\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.116848\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.065467\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.041913\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.073437\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.042181\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.092132\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.088941\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.113160\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.028048\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.041840\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.110604\n",
      "\n",
      "Test set: Average loss: 0.1158, Accuracy: 24118/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.057999\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.103609\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.132293\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.179445\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.067377\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.124831\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.061664\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.050511\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.110839\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.019164\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.033539\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.036177\n",
      "\n",
      "Test set: Average loss: 0.1163, Accuracy: 24129/25000 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.087079\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.049114\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.002543\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.048642\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.070435\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.068197\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.045298\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.045082\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.053568\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.034522\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.040574\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.049144\n",
      "\n",
      "Test set: Average loss: 0.1228, Accuracy: 24124/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.053429\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.010113\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.174930\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.019107\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.074860\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.036271\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.011382\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.014712\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.078903\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.025011\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.015207\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.168837\n",
      "\n",
      "Test set: Average loss: 0.1210, Accuracy: 24124/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.029753\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.106798\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.052854\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.068987\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.094274\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.031796\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.237417\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.038081\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.094847\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.038049\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.085710\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.170434\n",
      "\n",
      "Test set: Average loss: 0.1195, Accuracy: 24131/25000 (97%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.011342\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.007310\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.077744\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.071383\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.045143\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.038128\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.026918\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.037374\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.052109\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.010710\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.123596\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.058995\n",
      "\n",
      "Test set: Average loss: 0.1172, Accuracy: 24146/25000 (97%)\n",
      "\n",
      "Training model with 48 and 96 kernels\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 0.251816\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.091908\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.185738\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.016075\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.065113\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.040876\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.047381\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.139328\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.045003\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.211893\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.051824\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.133582\n",
      "\n",
      "Test set: Average loss: 0.1213, Accuracy: 24011/25000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.210218\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.081539\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.107879\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.105526\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.080826\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.047831\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.154247\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.163440\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.126373\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.125709\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.162367\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.104355\n",
      "\n",
      "Test set: Average loss: 0.1226, Accuracy: 24055/25000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.156505\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.162340\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.289738\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.080801\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.092338\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.146356\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.090130\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.104322\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.121475\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.011332\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.094839\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.140314\n",
      "\n",
      "Test set: Average loss: 0.1210, Accuracy: 24059/25000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.256016\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.076474\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.005363\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.127744\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.070434\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.056782\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.100989\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.030107\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.099740\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.097684\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.033430\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.066131\n",
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 24075/25000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.144741\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.146107\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.074165\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.072013\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.117183\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.160033\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.071097\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.034487\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.095111\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.073315\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.030104\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.090671\n",
      "\n",
      "Test set: Average loss: 0.1145, Accuracy: 24071/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.101294\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.104678\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.109309\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.105112\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.034881\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.046724\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.113014\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.186877\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.029827\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.121189\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.052960\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.149293\n",
      "\n",
      "Test set: Average loss: 0.1161, Accuracy: 24098/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.148065\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.170761\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.090031\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.082150\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.041432\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.105924\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.083725\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.089734\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.098854\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.230525\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.009301\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.077575\n",
      "\n",
      "Test set: Average loss: 0.1134, Accuracy: 24107/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.073490\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[173]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m testAcc = []\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     trainLoss.append(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m.item())\n\u001b[32m     25\u001b[39m     t,a = test(model, device, test_loader, test_criterion)\n\u001b[32m     26\u001b[39m     testLoss.append(t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[169]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, device, train_loader, criterion, optimizer, epoch)\u001b[39m\n\u001b[32m     13\u001b[39m loss = criterion(logits, target)\n\u001b[32m     14\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_idx % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTrain Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m)] Loss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m     19\u001b[39m         epoch, batch_idx * \u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mlen\u001b[39m(train_loader.dataset),\n\u001b[32m     20\u001b[39m         \u001b[32m100.\u001b[39m * batch_idx / \u001b[38;5;28mlen\u001b[39m(train_loader), loss.item()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/optim/lr_scheduler.py:124\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m opt = opt_ref()\n\u001b[32m    123\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/optim/optimizer.py:470\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28mself\u001b[39m = cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    469\u001b[39m profile_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/autograd/profiler.py:771\u001b[39m, in \u001b[36mrecord_function.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m     \u001b[38;5;28mself\u001b[39m.record = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/_ops.py:1158\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=6)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "# Training\n",
    "train_criterion = nn.CrossEntropyLoss() # CrossEntropyLoss combines log-softmax + NLLLoss\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum') # For test function\n",
    "\n",
    "for i in range(nets):\n",
    "    print(f\"Training model with {i*8+8} and {i*16+16} kernels\")\n",
    "    model = cnn_num_kernels_models[i].to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = LambdaLR(optimizer, lambda epoch: 0.95 ** epoch)\n",
    "    trainLoss = []\n",
    "    testLoss = []\n",
    "    testAcc = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        trainLoss.append(train(model, device, train_loader, train_criterion, optimizer, epoch).item())\n",
    "        t,a = test(model, device, test_loader, test_criterion)\n",
    "        testLoss.append(t)\n",
    "        testAcc.append(a)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5cdb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             832\n",
      "         LeakyReLU-2           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
      "            Conv2d-4           [-1, 64, 14, 14]          51,264\n",
      "         LeakyReLU-5           [-1, 64, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 64, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3136]               0\n",
      "            Linear-8                  [-1, 256]         803,072\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 856,453\n",
      "Trainable params: 856,453\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.67\n",
      "Params size (MB): 3.27\n",
      "Estimated Total Size (MB): 3.94\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             832\n",
      "         LeakyReLU-2           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
      "            Conv2d-4           [-1, 64, 14, 14]          51,264\n",
      "         LeakyReLU-5           [-1, 64, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 64, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3136]               0\n",
      "            Linear-8                  [-1, 256]         803,072\n",
      "         LeakyReLU-9                  [-1, 256]               0\n",
      "           Linear-10                  [-1, 128]          32,896\n",
      "        LeakyReLU-11                  [-1, 128]               0\n",
      "           Linear-12                    [-1, 5]             645\n",
      "================================================================\n",
      "Total params: 888,709\n",
      "Trainable params: 888,709\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.68\n",
      "Params size (MB): 3.39\n",
      "Estimated Total Size (MB): 4.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nets = 2\n",
    "cnn_num_dense_layer_models = [BaseModule() for _ in range(nets)]\n",
    "\n",
    "for i in range(nets):\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.Conv2d(1, 32, kernel_size=5, padding='same'))\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.Conv2d(32, 64, kernel_size=5, padding='same'))\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.Flatten())\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.Linear(64*7*7, 256))\n",
    "    cnn_num_dense_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "\n",
    "    if i == 0:\n",
    "        cnn_num_dense_layer_models[i].layers.append(nn.Linear(256, 5))\n",
    "    else:\n",
    "        cnn_num_dense_layer_models[i].layers.append(nn.Linear(256, 128))\n",
    "        cnn_num_dense_layer_models[i].layers.append(nn.LeakyReLU())\n",
    "        cnn_num_dense_layer_models[i].layers.append(nn.Linear(128, 5))\n",
    "\n",
    "# Print models\n",
    "for i in range(nets):\n",
    "    summary(cnn_num_dense_layer_models[i], input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8695169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 1 dense layers\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 1.613632\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.477903\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.424915\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.404655\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.364250\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.391702\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.446629\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.213461\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.285810\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.225441\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.160558\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.143415\n",
      "\n",
      "Test set: Average loss: 0.2036, Accuracy: 23334/25000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.147010\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.194148\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.317836\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.101981\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.195652\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.324563\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.325317\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.272765\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.269679\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.231284\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.229396\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.355423\n",
      "\n",
      "Test set: Average loss: 0.1829, Accuracy: 23473/25000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.229829\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.133837\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.192589\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.190376\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.271151\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.171697\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.296539\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.210102\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.187278\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.244713\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.244809\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.131538\n",
      "\n",
      "Test set: Average loss: 0.1405, Accuracy: 23803/25000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.190166\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.163804\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.251265\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.162631\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.204078\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.176494\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.088530\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.048984\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.178607\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.244888\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.135804\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.106738\n",
      "\n",
      "Test set: Average loss: 0.1374, Accuracy: 23841/25000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.108460\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.209558\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.236248\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.059397\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.102207\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.103109\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.122030\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.094367\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.193373\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.061355\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.133396\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.118723\n",
      "\n",
      "Test set: Average loss: 0.1435, Accuracy: 23797/25000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.138032\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.094740\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.204522\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.080832\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.262752\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.107502\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.160637\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.085999\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.121954\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.046928\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.153235\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.153671\n",
      "\n",
      "Test set: Average loss: 0.1247, Accuracy: 23916/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.066306\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.067779\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.099560\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.088721\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.211239\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.226241\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.161458\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.198713\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.090521\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.090927\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.116173\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.145616\n",
      "\n",
      "Test set: Average loss: 0.1218, Accuracy: 23975/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.210030\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.086111\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.059003\n",
      "Train Epoch: 8 [19200/75000 (26%)] Loss: 0.134877\n",
      "Train Epoch: 8 [25600/75000 (34%)] Loss: 0.313988\n",
      "Train Epoch: 8 [32000/75000 (43%)] Loss: 0.186975\n",
      "Train Epoch: 8 [38400/75000 (51%)] Loss: 0.157720\n",
      "Train Epoch: 8 [44800/75000 (60%)] Loss: 0.092918\n",
      "Train Epoch: 8 [51200/75000 (68%)] Loss: 0.091718\n",
      "Train Epoch: 8 [57600/75000 (77%)] Loss: 0.061046\n",
      "Train Epoch: 8 [64000/75000 (85%)] Loss: 0.085901\n",
      "Train Epoch: 8 [70400/75000 (94%)] Loss: 0.072417\n",
      "\n",
      "Test set: Average loss: 0.1187, Accuracy: 23988/25000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/75000 (0%)] Loss: 0.111967\n",
      "Train Epoch: 9 [6400/75000 (9%)] Loss: 0.073046\n",
      "Train Epoch: 9 [12800/75000 (17%)] Loss: 0.049293\n",
      "Train Epoch: 9 [19200/75000 (26%)] Loss: 0.087285\n",
      "Train Epoch: 9 [25600/75000 (34%)] Loss: 0.188693\n",
      "Train Epoch: 9 [32000/75000 (43%)] Loss: 0.298778\n",
      "Train Epoch: 9 [38400/75000 (51%)] Loss: 0.070363\n",
      "Train Epoch: 9 [44800/75000 (60%)] Loss: 0.042504\n",
      "Train Epoch: 9 [51200/75000 (68%)] Loss: 0.059644\n",
      "Train Epoch: 9 [57600/75000 (77%)] Loss: 0.058353\n",
      "Train Epoch: 9 [64000/75000 (85%)] Loss: 0.042600\n",
      "Train Epoch: 9 [70400/75000 (94%)] Loss: 0.201761\n",
      "\n",
      "Test set: Average loss: 0.1197, Accuracy: 24018/25000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/75000 (0%)] Loss: 0.090468\n",
      "Train Epoch: 10 [6400/75000 (9%)] Loss: 0.082248\n",
      "Train Epoch: 10 [12800/75000 (17%)] Loss: 0.159181\n",
      "Train Epoch: 10 [19200/75000 (26%)] Loss: 0.069503\n",
      "Train Epoch: 10 [25600/75000 (34%)] Loss: 0.121070\n",
      "Train Epoch: 10 [32000/75000 (43%)] Loss: 0.160461\n",
      "Train Epoch: 10 [38400/75000 (51%)] Loss: 0.206860\n",
      "Train Epoch: 10 [44800/75000 (60%)] Loss: 0.195436\n",
      "Train Epoch: 10 [51200/75000 (68%)] Loss: 0.173128\n",
      "Train Epoch: 10 [57600/75000 (77%)] Loss: 0.158296\n",
      "Train Epoch: 10 [64000/75000 (85%)] Loss: 0.131801\n",
      "Train Epoch: 10 [70400/75000 (94%)] Loss: 0.210057\n",
      "\n",
      "Test set: Average loss: 0.1193, Accuracy: 24031/25000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/75000 (0%)] Loss: 0.135277\n",
      "Train Epoch: 11 [6400/75000 (9%)] Loss: 0.023046\n",
      "Train Epoch: 11 [12800/75000 (17%)] Loss: 0.250759\n",
      "Train Epoch: 11 [19200/75000 (26%)] Loss: 0.187293\n",
      "Train Epoch: 11 [25600/75000 (34%)] Loss: 0.074250\n",
      "Train Epoch: 11 [32000/75000 (43%)] Loss: 0.039527\n",
      "Train Epoch: 11 [38400/75000 (51%)] Loss: 0.203502\n",
      "Train Epoch: 11 [44800/75000 (60%)] Loss: 0.061304\n",
      "Train Epoch: 11 [51200/75000 (68%)] Loss: 0.057913\n",
      "Train Epoch: 11 [57600/75000 (77%)] Loss: 0.066375\n",
      "Train Epoch: 11 [64000/75000 (85%)] Loss: 0.111545\n",
      "Train Epoch: 11 [70400/75000 (94%)] Loss: 0.230240\n",
      "\n",
      "Test set: Average loss: 0.1136, Accuracy: 24086/25000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/75000 (0%)] Loss: 0.087678\n",
      "Train Epoch: 12 [6400/75000 (9%)] Loss: 0.023470\n",
      "Train Epoch: 12 [12800/75000 (17%)] Loss: 0.068280\n",
      "Train Epoch: 12 [19200/75000 (26%)] Loss: 0.068807\n",
      "Train Epoch: 12 [25600/75000 (34%)] Loss: 0.090791\n",
      "Train Epoch: 12 [32000/75000 (43%)] Loss: 0.022991\n",
      "Train Epoch: 12 [38400/75000 (51%)] Loss: 0.028267\n",
      "Train Epoch: 12 [44800/75000 (60%)] Loss: 0.064545\n",
      "Train Epoch: 12 [51200/75000 (68%)] Loss: 0.053927\n",
      "Train Epoch: 12 [57600/75000 (77%)] Loss: 0.024246\n",
      "Train Epoch: 12 [64000/75000 (85%)] Loss: 0.063064\n",
      "Train Epoch: 12 [70400/75000 (94%)] Loss: 0.042055\n",
      "\n",
      "Test set: Average loss: 0.1161, Accuracy: 24086/25000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/75000 (0%)] Loss: 0.090788\n",
      "Train Epoch: 13 [6400/75000 (9%)] Loss: 0.131312\n",
      "Train Epoch: 13 [12800/75000 (17%)] Loss: 0.093634\n",
      "Train Epoch: 13 [19200/75000 (26%)] Loss: 0.089614\n",
      "Train Epoch: 13 [25600/75000 (34%)] Loss: 0.084266\n",
      "Train Epoch: 13 [32000/75000 (43%)] Loss: 0.119887\n",
      "Train Epoch: 13 [38400/75000 (51%)] Loss: 0.115573\n",
      "Train Epoch: 13 [44800/75000 (60%)] Loss: 0.022373\n",
      "Train Epoch: 13 [51200/75000 (68%)] Loss: 0.090323\n",
      "Train Epoch: 13 [57600/75000 (77%)] Loss: 0.141744\n",
      "Train Epoch: 13 [64000/75000 (85%)] Loss: 0.177315\n",
      "Train Epoch: 13 [70400/75000 (94%)] Loss: 0.083096\n",
      "\n",
      "Test set: Average loss: 0.1130, Accuracy: 24093/25000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/75000 (0%)] Loss: 0.045028\n",
      "Train Epoch: 14 [6400/75000 (9%)] Loss: 0.041472\n",
      "Train Epoch: 14 [12800/75000 (17%)] Loss: 0.064454\n",
      "Train Epoch: 14 [19200/75000 (26%)] Loss: 0.085859\n",
      "Train Epoch: 14 [25600/75000 (34%)] Loss: 0.105804\n",
      "Train Epoch: 14 [32000/75000 (43%)] Loss: 0.040852\n",
      "Train Epoch: 14 [38400/75000 (51%)] Loss: 0.098873\n",
      "Train Epoch: 14 [44800/75000 (60%)] Loss: 0.072030\n",
      "Train Epoch: 14 [51200/75000 (68%)] Loss: 0.107726\n",
      "Train Epoch: 14 [57600/75000 (77%)] Loss: 0.109900\n",
      "Train Epoch: 14 [64000/75000 (85%)] Loss: 0.259140\n",
      "Train Epoch: 14 [70400/75000 (94%)] Loss: 0.103857\n",
      "\n",
      "Test set: Average loss: 0.1149, Accuracy: 24050/25000 (96%)\n",
      "\n",
      "Train Epoch: 15 [0/75000 (0%)] Loss: 0.082451\n",
      "Train Epoch: 15 [6400/75000 (9%)] Loss: 0.064568\n",
      "Train Epoch: 15 [12800/75000 (17%)] Loss: 0.073705\n",
      "Train Epoch: 15 [19200/75000 (26%)] Loss: 0.036720\n",
      "Train Epoch: 15 [25600/75000 (34%)] Loss: 0.112154\n",
      "Train Epoch: 15 [32000/75000 (43%)] Loss: 0.101298\n",
      "Train Epoch: 15 [38400/75000 (51%)] Loss: 0.130982\n",
      "Train Epoch: 15 [44800/75000 (60%)] Loss: 0.084070\n",
      "Train Epoch: 15 [51200/75000 (68%)] Loss: 0.223236\n",
      "Train Epoch: 15 [57600/75000 (77%)] Loss: 0.123525\n",
      "Train Epoch: 15 [64000/75000 (85%)] Loss: 0.131016\n",
      "Train Epoch: 15 [70400/75000 (94%)] Loss: 0.065567\n",
      "\n",
      "Test set: Average loss: 0.1128, Accuracy: 24087/25000 (96%)\n",
      "\n",
      "Train Epoch: 16 [0/75000 (0%)] Loss: 0.088098\n",
      "Train Epoch: 16 [6400/75000 (9%)] Loss: 0.082539\n",
      "Train Epoch: 16 [12800/75000 (17%)] Loss: 0.109252\n",
      "Train Epoch: 16 [19200/75000 (26%)] Loss: 0.048022\n",
      "Train Epoch: 16 [25600/75000 (34%)] Loss: 0.154317\n",
      "Train Epoch: 16 [32000/75000 (43%)] Loss: 0.088578\n",
      "Train Epoch: 16 [38400/75000 (51%)] Loss: 0.041948\n",
      "Train Epoch: 16 [44800/75000 (60%)] Loss: 0.203003\n",
      "Train Epoch: 16 [51200/75000 (68%)] Loss: 0.120200\n",
      "Train Epoch: 16 [57600/75000 (77%)] Loss: 0.047003\n",
      "Train Epoch: 16 [64000/75000 (85%)] Loss: 0.084881\n",
      "Train Epoch: 16 [70400/75000 (94%)] Loss: 0.084847\n",
      "\n",
      "Test set: Average loss: 0.1123, Accuracy: 24080/25000 (96%)\n",
      "\n",
      "Train Epoch: 17 [0/75000 (0%)] Loss: 0.101697\n",
      "Train Epoch: 17 [6400/75000 (9%)] Loss: 0.181024\n",
      "Train Epoch: 17 [12800/75000 (17%)] Loss: 0.066278\n",
      "Train Epoch: 17 [19200/75000 (26%)] Loss: 0.048658\n",
      "Train Epoch: 17 [25600/75000 (34%)] Loss: 0.119784\n",
      "Train Epoch: 17 [32000/75000 (43%)] Loss: 0.115126\n",
      "Train Epoch: 17 [38400/75000 (51%)] Loss: 0.034518\n",
      "Train Epoch: 17 [44800/75000 (60%)] Loss: 0.061862\n",
      "Train Epoch: 17 [51200/75000 (68%)] Loss: 0.074563\n",
      "Train Epoch: 17 [57600/75000 (77%)] Loss: 0.041811\n",
      "Train Epoch: 17 [64000/75000 (85%)] Loss: 0.043581\n",
      "Train Epoch: 17 [70400/75000 (94%)] Loss: 0.131163\n",
      "\n",
      "Test set: Average loss: 0.1143, Accuracy: 24077/25000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/75000 (0%)] Loss: 0.221548\n",
      "Train Epoch: 18 [6400/75000 (9%)] Loss: 0.067985\n",
      "Train Epoch: 18 [12800/75000 (17%)] Loss: 0.229062\n",
      "Train Epoch: 18 [19200/75000 (26%)] Loss: 0.049255\n",
      "Train Epoch: 18 [25600/75000 (34%)] Loss: 0.012491\n",
      "Train Epoch: 18 [32000/75000 (43%)] Loss: 0.058752\n",
      "Train Epoch: 18 [38400/75000 (51%)] Loss: 0.016523\n",
      "Train Epoch: 18 [44800/75000 (60%)] Loss: 0.018627\n",
      "Train Epoch: 18 [51200/75000 (68%)] Loss: 0.157796\n",
      "Train Epoch: 18 [57600/75000 (77%)] Loss: 0.042273\n",
      "Train Epoch: 18 [64000/75000 (85%)] Loss: 0.019165\n",
      "Train Epoch: 18 [70400/75000 (94%)] Loss: 0.077483\n",
      "\n",
      "Test set: Average loss: 0.1149, Accuracy: 24102/25000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/75000 (0%)] Loss: 0.107435\n",
      "Train Epoch: 19 [6400/75000 (9%)] Loss: 0.092790\n",
      "Train Epoch: 19 [12800/75000 (17%)] Loss: 0.074272\n",
      "Train Epoch: 19 [19200/75000 (26%)] Loss: 0.129247\n",
      "Train Epoch: 19 [25600/75000 (34%)] Loss: 0.184560\n",
      "Train Epoch: 19 [32000/75000 (43%)] Loss: 0.059472\n",
      "Train Epoch: 19 [38400/75000 (51%)] Loss: 0.163807\n",
      "Train Epoch: 19 [44800/75000 (60%)] Loss: 0.060347\n",
      "Train Epoch: 19 [51200/75000 (68%)] Loss: 0.144715\n",
      "Train Epoch: 19 [57600/75000 (77%)] Loss: 0.111398\n",
      "Train Epoch: 19 [64000/75000 (85%)] Loss: 0.030578\n",
      "Train Epoch: 19 [70400/75000 (94%)] Loss: 0.099722\n",
      "\n",
      "Test set: Average loss: 0.1166, Accuracy: 24085/25000 (96%)\n",
      "\n",
      "Train Epoch: 20 [0/75000 (0%)] Loss: 0.075975\n",
      "Train Epoch: 20 [6400/75000 (9%)] Loss: 0.083051\n",
      "Train Epoch: 20 [12800/75000 (17%)] Loss: 0.111011\n",
      "Train Epoch: 20 [19200/75000 (26%)] Loss: 0.040203\n",
      "Train Epoch: 20 [25600/75000 (34%)] Loss: 0.147735\n",
      "Train Epoch: 20 [32000/75000 (43%)] Loss: 0.177297\n",
      "Train Epoch: 20 [38400/75000 (51%)] Loss: 0.080505\n",
      "Train Epoch: 20 [44800/75000 (60%)] Loss: 0.064353\n",
      "Train Epoch: 20 [51200/75000 (68%)] Loss: 0.024907\n",
      "Train Epoch: 20 [57600/75000 (77%)] Loss: 0.081992\n",
      "Train Epoch: 20 [64000/75000 (85%)] Loss: 0.111420\n",
      "Train Epoch: 20 [70400/75000 (94%)] Loss: 0.167253\n",
      "\n",
      "Test set: Average loss: 0.1109, Accuracy: 24095/25000 (96%)\n",
      "\n",
      "Training model with 2 dense layers\n",
      "Train Epoch: 1 [0/75000 (0%)] Loss: 1.615527\n",
      "Train Epoch: 1 [6400/75000 (9%)] Loss: 0.490856\n",
      "Train Epoch: 1 [12800/75000 (17%)] Loss: 0.366632\n",
      "Train Epoch: 1 [19200/75000 (26%)] Loss: 0.404100\n",
      "Train Epoch: 1 [25600/75000 (34%)] Loss: 0.387533\n",
      "Train Epoch: 1 [32000/75000 (43%)] Loss: 0.629421\n",
      "Train Epoch: 1 [38400/75000 (51%)] Loss: 0.276036\n",
      "Train Epoch: 1 [44800/75000 (60%)] Loss: 0.345421\n",
      "Train Epoch: 1 [51200/75000 (68%)] Loss: 0.128223\n",
      "Train Epoch: 1 [57600/75000 (77%)] Loss: 0.297615\n",
      "Train Epoch: 1 [64000/75000 (85%)] Loss: 0.350390\n",
      "Train Epoch: 1 [70400/75000 (94%)] Loss: 0.186934\n",
      "\n",
      "Test set: Average loss: 0.1903, Accuracy: 23393/25000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/75000 (0%)] Loss: 0.196651\n",
      "Train Epoch: 2 [6400/75000 (9%)] Loss: 0.289714\n",
      "Train Epoch: 2 [12800/75000 (17%)] Loss: 0.244614\n",
      "Train Epoch: 2 [19200/75000 (26%)] Loss: 0.210699\n",
      "Train Epoch: 2 [25600/75000 (34%)] Loss: 0.231048\n",
      "Train Epoch: 2 [32000/75000 (43%)] Loss: 0.194847\n",
      "Train Epoch: 2 [38400/75000 (51%)] Loss: 0.369433\n",
      "Train Epoch: 2 [44800/75000 (60%)] Loss: 0.051694\n",
      "Train Epoch: 2 [51200/75000 (68%)] Loss: 0.386294\n",
      "Train Epoch: 2 [57600/75000 (77%)] Loss: 0.135724\n",
      "Train Epoch: 2 [64000/75000 (85%)] Loss: 0.215555\n",
      "Train Epoch: 2 [70400/75000 (94%)] Loss: 0.235820\n",
      "\n",
      "Test set: Average loss: 0.1549, Accuracy: 23671/25000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/75000 (0%)] Loss: 0.151126\n",
      "Train Epoch: 3 [6400/75000 (9%)] Loss: 0.298697\n",
      "Train Epoch: 3 [12800/75000 (17%)] Loss: 0.172767\n",
      "Train Epoch: 3 [19200/75000 (26%)] Loss: 0.159749\n",
      "Train Epoch: 3 [25600/75000 (34%)] Loss: 0.188537\n",
      "Train Epoch: 3 [32000/75000 (43%)] Loss: 0.118438\n",
      "Train Epoch: 3 [38400/75000 (51%)] Loss: 0.150547\n",
      "Train Epoch: 3 [44800/75000 (60%)] Loss: 0.318001\n",
      "Train Epoch: 3 [51200/75000 (68%)] Loss: 0.262448\n",
      "Train Epoch: 3 [57600/75000 (77%)] Loss: 0.136963\n",
      "Train Epoch: 3 [64000/75000 (85%)] Loss: 0.123662\n",
      "Train Epoch: 3 [70400/75000 (94%)] Loss: 0.105608\n",
      "\n",
      "Test set: Average loss: 0.1402, Accuracy: 23800/25000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/75000 (0%)] Loss: 0.079764\n",
      "Train Epoch: 4 [6400/75000 (9%)] Loss: 0.103901\n",
      "Train Epoch: 4 [12800/75000 (17%)] Loss: 0.249326\n",
      "Train Epoch: 4 [19200/75000 (26%)] Loss: 0.102416\n",
      "Train Epoch: 4 [25600/75000 (34%)] Loss: 0.109013\n",
      "Train Epoch: 4 [32000/75000 (43%)] Loss: 0.197036\n",
      "Train Epoch: 4 [38400/75000 (51%)] Loss: 0.104349\n",
      "Train Epoch: 4 [44800/75000 (60%)] Loss: 0.153458\n",
      "Train Epoch: 4 [51200/75000 (68%)] Loss: 0.475075\n",
      "Train Epoch: 4 [57600/75000 (77%)] Loss: 0.192364\n",
      "Train Epoch: 4 [64000/75000 (85%)] Loss: 0.185292\n",
      "Train Epoch: 4 [70400/75000 (94%)] Loss: 0.149607\n",
      "\n",
      "Test set: Average loss: 0.1352, Accuracy: 23867/25000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/75000 (0%)] Loss: 0.157324\n",
      "Train Epoch: 5 [6400/75000 (9%)] Loss: 0.165787\n",
      "Train Epoch: 5 [12800/75000 (17%)] Loss: 0.134739\n",
      "Train Epoch: 5 [19200/75000 (26%)] Loss: 0.076992\n",
      "Train Epoch: 5 [25600/75000 (34%)] Loss: 0.091149\n",
      "Train Epoch: 5 [32000/75000 (43%)] Loss: 0.282001\n",
      "Train Epoch: 5 [38400/75000 (51%)] Loss: 0.285558\n",
      "Train Epoch: 5 [44800/75000 (60%)] Loss: 0.038059\n",
      "Train Epoch: 5 [51200/75000 (68%)] Loss: 0.079229\n",
      "Train Epoch: 5 [57600/75000 (77%)] Loss: 0.133414\n",
      "Train Epoch: 5 [64000/75000 (85%)] Loss: 0.172702\n",
      "Train Epoch: 5 [70400/75000 (94%)] Loss: 0.101243\n",
      "\n",
      "Test set: Average loss: 0.1312, Accuracy: 23891/25000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/75000 (0%)] Loss: 0.194774\n",
      "Train Epoch: 6 [6400/75000 (9%)] Loss: 0.187167\n",
      "Train Epoch: 6 [12800/75000 (17%)] Loss: 0.153759\n",
      "Train Epoch: 6 [19200/75000 (26%)] Loss: 0.352185\n",
      "Train Epoch: 6 [25600/75000 (34%)] Loss: 0.128602\n",
      "Train Epoch: 6 [32000/75000 (43%)] Loss: 0.060705\n",
      "Train Epoch: 6 [38400/75000 (51%)] Loss: 0.149974\n",
      "Train Epoch: 6 [44800/75000 (60%)] Loss: 0.186332\n",
      "Train Epoch: 6 [51200/75000 (68%)] Loss: 0.043441\n",
      "Train Epoch: 6 [57600/75000 (77%)] Loss: 0.168037\n",
      "Train Epoch: 6 [64000/75000 (85%)] Loss: 0.106592\n",
      "Train Epoch: 6 [70400/75000 (94%)] Loss: 0.085027\n",
      "\n",
      "Test set: Average loss: 0.1245, Accuracy: 23929/25000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/75000 (0%)] Loss: 0.184795\n",
      "Train Epoch: 7 [6400/75000 (9%)] Loss: 0.222902\n",
      "Train Epoch: 7 [12800/75000 (17%)] Loss: 0.099292\n",
      "Train Epoch: 7 [19200/75000 (26%)] Loss: 0.137096\n",
      "Train Epoch: 7 [25600/75000 (34%)] Loss: 0.066962\n",
      "Train Epoch: 7 [32000/75000 (43%)] Loss: 0.169206\n",
      "Train Epoch: 7 [38400/75000 (51%)] Loss: 0.196794\n",
      "Train Epoch: 7 [44800/75000 (60%)] Loss: 0.158907\n",
      "Train Epoch: 7 [51200/75000 (68%)] Loss: 0.078758\n",
      "Train Epoch: 7 [57600/75000 (77%)] Loss: 0.059952\n",
      "Train Epoch: 7 [64000/75000 (85%)] Loss: 0.124700\n",
      "Train Epoch: 7 [70400/75000 (94%)] Loss: 0.123573\n",
      "\n",
      "Test set: Average loss: 0.1298, Accuracy: 23886/25000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/75000 (0%)] Loss: 0.120604\n",
      "Train Epoch: 8 [6400/75000 (9%)] Loss: 0.148136\n",
      "Train Epoch: 8 [12800/75000 (17%)] Loss: 0.200918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m testAcc = []\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     trainLoss.append(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m.item())\n\u001b[32m     25\u001b[39m     t,a = test(model, device, test_loader, test_criterion)\n\u001b[32m     26\u001b[39m     testLoss.append(t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, device, train_loader, criterion, optimizer, epoch)\u001b[39m\n\u001b[32m     12\u001b[39m logits = model(data)\n\u001b[32m     13\u001b[39m loss = criterion(logits, target)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m optimizer.step()\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_idx % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/edu/bachelor/it6/dl/venv/lib64/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=6)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "# Training\n",
    "train_criterion = nn.CrossEntropyLoss() # CrossEntropyLoss combines log-softmax + NLLLoss\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum') # For test function\n",
    "\n",
    "for i in range(nets):\n",
    "    print(f\"Training model with {i+1} dense layers\")\n",
    "    model = cnn_num_dense_layer_models[i].to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = LambdaLR(optimizer, lambda epoch: 0.95 ** epoch)\n",
    "    trainLoss = []\n",
    "    testLoss = []\n",
    "    testAcc = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        trainLoss.append(train(model, device, train_loader, train_criterion, optimizer, epoch).item())\n",
    "        t,a = test(model, device, test_loader, test_criterion)\n",
    "        testLoss.append(t)\n",
    "        testAcc.append(a)\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
